{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net Implementation of Monocular Depth Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda enabled on device: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import functools\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Set Device\n",
    "    torch.cuda.set_device(0)\n",
    "    print(\"Cuda enabled on device: {}\".format(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile depthnet/model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from collections import OrderedDict\n",
    "\n",
    "def create_sublayer_dict(input_nc, output_nc, layer_index, nconvs, norm_layer, **conv_kwargs):\n",
    "    sublayer=OrderedDict()\n",
    "    j = 0\n",
    "    sublayer.update({\"conv{}_{}\".format(layer_index, j): nn.Conv2d(input_nc, output_nc, **conv_kwargs)})\n",
    "    j += 1\n",
    "    sublayer.update({\"relu{}_{}\".format(layer_index, j): nn.ReLU(True)})\n",
    "    for i in range(nconvs-1):\n",
    "        j += 1\n",
    "        sublayer.update({\"conv{}_{}\".format(layer_index, j): nn.Conv2d(output_nc, output_nc, **conv_kwargs)})\n",
    "        j += 1\n",
    "        sublayer.update({\"relu{}_{}\".format(layer_index, j): nn.ReLU(True)})\n",
    "    if norm_layer:\n",
    "        j += 1\n",
    "        sublayer.update({\"norm{}_{}\".format(layer_index, j): norm_layer(output_nc)})\n",
    "    return sublayer\n",
    "\n",
    "class DepthNet(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, norm_layer=nn.BatchNorm2d):\n",
    "        super(DepthNet, self).__init__()\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        use_bias = True\n",
    "\n",
    "        # Conv1\n",
    "        model1 = create_sublayer_dict(input_nc, 32, 1, 2, norm_layer, \n",
    "                                      kernel_size=3, stride=1, padding=1, bias=use_bias)\n",
    "        # add a subsampling operation (in self.forward())\n",
    "\n",
    "        # Conv2\n",
    "        model2 = create_sublayer_dict(32, 64, 2, 2, norm_layer,\n",
    "                                      kernel_size=3, stride=1, padding=1, bias=use_bias)\n",
    "        # add a subsampling layer operation (in self.forward())\n",
    "\n",
    "        # Conv3\n",
    "        model3 = create_sublayer_dict(64, 128, 3, 3, norm_layer,\n",
    "                                      kernel_size=3, stride=1, padding=1, bias=use_bias)\n",
    "        # add a subsampling layer operation\n",
    "\n",
    "        # Conv4\n",
    "        model4 = create_sublayer_dict(128, 256, 4, 3, norm_layer,\n",
    "                                      kernel_size=3, stride=1, padding=1, bias=use_bias)\n",
    "\n",
    "        # Conv5\n",
    "        model5 = create_sublayer_dict(256, 256, 5, 3, norm_layer,\n",
    "                                      kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias)\n",
    "\n",
    "        # Conv6\n",
    "        model6 = create_sublayer_dict(256, 256, 6, 3, norm_layer,\n",
    "                                      kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias)\n",
    "\n",
    "        # Conv7\n",
    "        model7 = create_sublayer_dict(256, 256, 7, 3, norm_layer,\n",
    "                                      kernel_size=3, stride=1, padding=1, bias=use_bias)\n",
    "\n",
    "        # Conv7\n",
    "        model8up=OrderedDict([(\"convt8_up\", \n",
    "                               nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=use_bias))]\n",
    "                            )\n",
    "        model3short8=OrderedDict([(\"conv3short8\",\n",
    "                                   nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias))]\n",
    "                                )\n",
    "\n",
    "        model8=OrderedDict([(\"relu8_pre\", nn.ReLU(True))])\n",
    "\n",
    "        model8.update(create_sublayer_dict(128, 128, 8, 2, norm_layer,\n",
    "                                      kernel_size=3, stride=1, padding=1, bias=use_bias))\n",
    "\n",
    "        # Conv9\n",
    "        model9up=OrderedDict([(\"convt9_up\", \n",
    "                               nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=use_bias))]\n",
    "                            )\n",
    "        model2short9=OrderedDict([(\"conv2short9\",\n",
    "                                   nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=use_bias))]\n",
    "                                )\n",
    "        # add the two feature maps above        \n",
    "\n",
    "        model9=OrderedDict([(\"relu9_pre\", nn.ReLU(True))])\n",
    "\n",
    "        model9.update(create_sublayer_dict(64, 64, 9, 1, norm_layer,\n",
    "                                           kernel_size=3, stride=1, padding=1, bias=use_bias))\n",
    "\n",
    "        # Conv10\n",
    "        model10up=OrderedDict([(\"conv10_up\", \n",
    "                               nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1, bias=use_bias))]\n",
    "                             )\n",
    "        model1short10=OrderedDict([(\"conv1short10\", \n",
    "                                    nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=use_bias))]\n",
    "                                 )\n",
    "        # add the two feature maps above\n",
    "\n",
    "        model10=OrderedDict([(\"relu10_pre\", nn.ReLU(True))])\n",
    "        model10.update({\"conv10_0\": nn.Conv2d(64, 64, kernel_size=3, dilation=1, stride=1, padding=1, bias=use_bias)})\n",
    "        model10.update({\"leakyrelu10_1\": nn.LeakyReLU(negative_slope=.2)})\n",
    "\n",
    "        # Depth Map Regression Output\n",
    "        model_out=OrderedDict([(\"conv_out\",\n",
    "                                nn.Conv2d(64, 1, kernel_size=1, padding=0, dilation=1, stride=1, bias=use_bias))]\n",
    "                             )\n",
    "        model_out.update({\"relu_out\": nn.ReLU(True)}) # Depth should be in [0, +inf)\n",
    "\n",
    "        self.model1 = nn.Sequential(model1)\n",
    "        self.model2 = nn.Sequential(model2)\n",
    "        self.model3 = nn.Sequential(model3)\n",
    "        self.model4 = nn.Sequential(model4)\n",
    "        self.model5 = nn.Sequential(model5)\n",
    "        self.model6 = nn.Sequential(model6)\n",
    "        self.model7 = nn.Sequential(model7)\n",
    "        self.model8up = nn.Sequential(model8up)\n",
    "        self.model8 = nn.Sequential(model8)\n",
    "        self.model9up = nn.Sequential(model9up)\n",
    "        self.model9 = nn.Sequential(model9)\n",
    "        self.model10up = nn.Sequential(model10up)\n",
    "        self.model10 = nn.Sequential(model10)\n",
    "        self.model3short8 = nn.Sequential(model3short8)\n",
    "        self.model2short9 = nn.Sequential(model2short9)\n",
    "        self.model1short10 = nn.Sequential(model1short10)\n",
    "        self.model_out = nn.Sequential(model_out)\n",
    "\n",
    "    def forward(self, input_A):\n",
    "        conv1_2 = self.model1(input_A)\n",
    "        conv2_2 = self.model2(conv1_2[:,:,::2,::2]) # downsample\n",
    "        conv3_3 = self.model3(conv2_2[:,:,::2,::2]) # downsample\n",
    "        conv4_3 = self.model4(conv3_3[:,:,::2,::2]) # downsample\n",
    "        conv5_3 = self.model5(conv4_3)\n",
    "        conv6_3 = self.model6(conv5_3)\n",
    "        conv7_3 = self.model7(conv6_3)\n",
    "        conv8_up = self.model8up(conv7_3) + self.model3short8(conv3_3) # Shortcut\n",
    "        conv8_3 = self.model8(conv8_up)\n",
    "        conv9_up = self.model9up(conv8_3) + self.model2short9(conv2_2) # Shortcut\n",
    "        conv9_3 = self.model9(conv9_up)\n",
    "        conv10_up = self.model10up(conv9_3) + self.model1short10(conv1_2) # Shortcut\n",
    "        conv10_2 = self.model10(conv10_up)\n",
    "        out_reg = self.model_out(conv10_2)\n",
    "\n",
    "        return out_reg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile depthnet/dataset.py\n",
    "###########\n",
    "# Dataset #\n",
    "###########\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import csv, numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from torchvision import transforms\n",
    "\n",
    "class DepthDataset(Dataset):\n",
    "    \"\"\"Class for reading and storing image and depth data together.\n",
    "    \"\"\"\n",
    "    def __init__(self, splitfile, dataDir, transform=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        images : list of (string, string)\n",
    "            list of (depth_map_path, rgb_path) filepaths to depth maps and their rgb images.\n",
    "        load_depth_map : function\n",
    "            the function for loading this particular kind of depth_map\n",
    "        load_rgb : function\n",
    "            the function for loading this particular kind of image.\n",
    "        \"\"\"\n",
    "        super(DepthDataset, self).__init__()\n",
    "        self.dataDir = dataDir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        with open(splitfile, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                self.data.append(line.strip().split(\",\"))\n",
    "#         print(self.data)\n",
    "    \n",
    "    def get_global_stats(self, outFile=None, writeFile=False):\n",
    "        \"\"\"Calculate mean and variance of each rgb channel.\n",
    "        \n",
    "        Optionally caches the result of this calculation in outfile so it doesn't need to be done each\n",
    "        time the dataset is loaded.\n",
    "        \"\"\"\n",
    "        S = np.zeros(3)\n",
    "        S_sq = np.zeros(3)\n",
    "        npixels = 0.\n",
    "        for depthFile, rgbFile in self.data:\n",
    "            rgbImg = Image.open(os.path.join(self.dataDir, rgbFile))\n",
    "            rgbImg = np.asarray(rgbImg, dtype=np.uint16)\n",
    "#             print(rgbImg[0:10, 0:10, :])\n",
    "            \n",
    "            npixels += rgbImg.shape[0]*rgbImg.shape[1]\n",
    "            for channel in range(rgbImg.shape[2]):\n",
    "                S[channel] += np.sum(rgbImg[:,:,channel])\n",
    "                S_sq[channel] += np.sum((rgbImg[:,:,channel])**2)\n",
    "        mean = S/npixels\n",
    "        var = S_sq/npixels - mean**2\n",
    "        \n",
    "        # Load full dataset (memory-intensive)\n",
    "#         full = []\n",
    "#         for depthFile, rgbFile in self.data:\n",
    "#             rgbImg = Image.open(os.path.join(self.dataDir, rgbFile))\n",
    "#             rgbImg = np.asarray(rgbImg, dtype=np.uint16)\n",
    "#             full.append(rgbImg)\n",
    "            \n",
    "#         a = np.array(full)\n",
    "#         mean_true = np.mean(a, axis=(0, 1, 2))\n",
    "#         var_true = np.var(a, axis=(0, 1, 2))\n",
    "#         print(\"actual mean and variance: {} {}\".format(mean_true, var_true))\n",
    "#         print(a.shape)\n",
    "        return mean, var\n",
    "                \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        depthFile, rgbFile = self.data[idx]\n",
    "        depthImg = Image.open(os.path.join(self.dataDir, depthFile))\n",
    "        rgbImg = Image.open(os.path.join(self.dataDir, rgbFile))\n",
    "        sample = {\"depth\": depthImg, \"rgb\": rgbImg}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "        \n",
    "##############\n",
    "# Transforms #\n",
    "##############\n",
    "# for data augmentation\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        depth, rgb = sample['depth'], sample['rgb']\n",
    "\n",
    "        h, w = depth.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        depth = depth[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "        \n",
    "        rgb = rgb[top: top + new_h,\n",
    "                  left: left + new_w]\n",
    "\n",
    "        return {'depth': depth, 'rgb': rgb}\n",
    "\n",
    "class CenterCrop(object):\n",
    "    \"\"\"Center crop the image\n",
    "    \n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "            \n",
    "    def __call__(self, sample):\n",
    "        depth, rgb = sample['depth'], sample['rgb']\n",
    "        h, w = depth.shape\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = h//2 - new_h//2\n",
    "        bottom = h//2 + new_h//2 + (1 if new_h % 2 else 0)\n",
    "        left = w//2 - new_w//2\n",
    "        right = w//2 + new_w//2 + (1 if new_w % 2 else 0)\n",
    "        \n",
    "        return {\"depth\": depth[top:bottom, left:right],\n",
    "                \"rgb\": rgb[top:bottom, left:right, :]}\n",
    "    \n",
    "class Crop_8(object):\n",
    "    \"\"\"Crop to a size where both dimensions are divisible by 8\"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        depth, rgb = sample['depth'], sample['rgb']\n",
    "        new_h, new_w = (depth.shape[0]//8)*8, (depth.shape[1]//8)*8\n",
    "        return {\"depth\": depth[:new_h, :new_w],\n",
    "                \"rgb\": rgb[:new_h, :new_w, :]}\n",
    "        \n",
    "class Crop_small(object):\n",
    "    def __call__(self, sample):\n",
    "        depth, rgb = sample['depth'], sample['rgb']\n",
    "        h, w = depth.shape[:2]\n",
    "        x = 16\n",
    "        return {\"depth\": depth[h//2-x:h//2+x, w//2-x:w//2+x],\n",
    "                \"rgb\": rgb[h//2-x:h//2+x, w//2-x:w//2+x, :]}\n",
    "\n",
    "class ToFloat(object):\n",
    "    \"\"\"Also parses the depth info for sunrgbd.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        depth = sample['depth']\n",
    "        x = np.asarray(depth, dtype=np.uint16)\n",
    "        y = (x >> 3) | (x << 16-3)\n",
    "        z = y.astype(np.float32)/1000\n",
    "        z[z>8.] = 8. # Clip to maximum depth of 8m.\n",
    "        return {\"depth\": z,\n",
    "                \"rgb\": np.asarray(sample['rgb']).astype(np.float32)}\n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        depth, rgb = sample['depth'], sample['rgb']\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "#         depth = depth.transpose((2, 0, 1))\n",
    "        rgb = rgb.transpose((2, 0, 1))            \n",
    "        output = {}\n",
    "        if 'hist' in sample:\n",
    "            output['hist'] = torch.from_numpy(sample['hist']).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "#         print(output)\n",
    "        output.update({'depth': torch.from_numpy(depth).unsqueeze(0),\n",
    "                'rgb': torch.from_numpy(rgb)})\n",
    "        return output\n",
    "    \n",
    "class AddDepthHist(object):\n",
    "    \"\"\"Takes a depth map and computes a histogram of depths as well\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        kwargs - passthrough to np.histogram\n",
    "        \"\"\"\n",
    "        self.hist_kwargs = kwargs\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        depth = sample[\"depth\"]\n",
    "        hist, _ = np.histogram(depth, **self.hist_kwargs)\n",
    "#         print(hist)\n",
    "#         print(sample[\"depth\"])\n",
    "        return {\"depth\": sample[\"depth\"],\n",
    "                \"rgb\": sample[\"rgb\"],\n",
    "                \"hist\": hist}\n",
    "\n",
    "class NormalizeRGB(object):\n",
    "    def __init__(self, mean, var):\n",
    "        \"\"\"\n",
    "        mean - np.array of size 3 - the means of the three color channels over the whole (training) dataset\n",
    "        var - np.array of size 3 - the variances of the three color channels over the whole (training) dataset\n",
    "        \"\"\"\n",
    "        self.mean = mean\n",
    "        self.var = var\n",
    "    def __call__(self, sample):\n",
    "        sample[\"rgb\"] -= self.mean\n",
    "        sample[\"rgb\"] /= np.sqrt(self.var)\n",
    "#         print(sample[\"rgb\"][0:10, 0:10, 0])\n",
    "        return sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile depthnet/utils.py\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.cuda as cuda\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import os.path\n",
    "# Helper functions\n",
    "\n",
    "#################\n",
    "# Checkpointing #\n",
    "#################\n",
    "def save_checkpoint(state, is_best, filename='/output/checkpoint.pth.tar', always_save=False):\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    if is_best or always_save:\n",
    "        print (\"=> Saving checkpoint to: {}\".format(filename))\n",
    "        torch.save(state, filename)  # save checkpoint\n",
    "    else:\n",
    "        print (\"=> Validation Accuracy did not improve\")\n",
    "\n",
    "##############\n",
    "# Validation #\n",
    "##############\n",
    "def validate(loss, model, val_loader):\n",
    "    \"\"\"Computes the validation error of the model on the validation set.\n",
    "    val_loader should be a DataLoader.\n",
    "    \n",
    "    Returns an ordinary number (i.e. not a tensor)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    it = None\n",
    "    losses = []\n",
    "    for it, data in enumerate(val_loader):\n",
    "        depth = data[\"depth\"].float()\n",
    "        rgb = data[\"rgb\"].float()\n",
    "        if torch.cuda.is_available():\n",
    "            depth = depth.cuda()\n",
    "            rgb = rgb.cuda()\n",
    "        if \"hist\" in data:\n",
    "#             print(data)\n",
    "            hist = data[\"hist\"].float()\n",
    "            if torch.cuda.is_available():\n",
    "                hist = hist.cuda()\n",
    "#             print(hist)\n",
    "            output = model(rgb, hist)\n",
    "        else:\n",
    "            output = model(rgb)\n",
    "        losses.append(loss(output, depth).item())\n",
    "    nbatches = it+1\n",
    "    return sum(losses)/nbatches\n",
    "\n",
    "##################\n",
    "# Viewing Images #\n",
    "##################\n",
    "def save_images(*batches, outputDir, filename):\n",
    "    \"\"\"\n",
    "    Given a list of tensors of size (B, C, H, W) (batch, channels, height, width) torch.Tensor\n",
    "    Saves each entry of the batch as an rgb or grayscale image, depending on how many channels\n",
    "    the image has.\n",
    "    \"\"\"\n",
    "    I = None\n",
    "    trans = transforms.ToPILImage()\n",
    "    for batchnum, batch in enumerate(batches):\n",
    "        if batch.shape[1] == 3:\n",
    "            pass\n",
    "        elif batch.shape[1] == 1:\n",
    "            batch /= torch.max(batch) # normalize to lie in [0, 1]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported number of channels: {}\".format(batch.shape[1]))\n",
    "        batch = batch.type(torch.float32)\n",
    "        for img in range(batch.shape[0]):            \n",
    "            I = trans(batch[img,:,:,:].cpu().detach())\n",
    "            I.save(os.path.join(outputDir, filename + \"_{}_{}.png\".format(batchnum, img)))\n",
    "\n",
    "\n",
    "############\n",
    "# Plotting #\n",
    "############\n",
    "def save_train_val_loss_plots(trainlosses, vallosses, epoch):\n",
    "    # Train loss\n",
    "    fig = plt.figure()\n",
    "    plt.plot(trainlosses)\n",
    "    plt.title(\"Train loss\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(\"trainloss_epoch{}.png\".format(epoch))\n",
    "    # Train loss\n",
    "    fig = plt.figure()\n",
    "    plt.plot(trainlosses)\n",
    "    plt.title(\"Val loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(\"Val loss{}.png\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training dataset from data/sunrgbd_nyu/train.txt with size 1159.\n",
      "Loaded dev dataset from data/sunrgbd_nyu/dev.txt with size 145.\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train_txt = \"data/sunrgbd_nyu/train.txt\"\n",
    "trainDir = \"data/sunrgbd_nyu\"\n",
    "train = DepthDataset(train_txt, trainDir)\n",
    "mean, var = train.get_global_stats()\n",
    "train.transform = transforms.Compose([ToFloat(),\n",
    "#                                       RandomCrop((400, 320)),\n",
    "                                      CenterCrop((320, 400)),\n",
    "                                      NormalizeRGB(mean, var),\n",
    "                                      ToTensor()\n",
    "                                      ])\n",
    "\n",
    "\n",
    "#                      transform=transforms.Compose([ToFloat(), Crop_8(), ToFloat(), ToTensor()])\n",
    "#                      transform=transforms.Compose([ToFloat(), Crop_small(), ToFloat(), ToTensor()])\n",
    "# print(mean, var)\n",
    "\n",
    "print(\"Loaded training dataset from {} with size {}.\".format(train_txt, len(train)))\n",
    "\n",
    "dev_txt = \"data/sunrgbd_nyu/dev.txt\"\n",
    "devDir = \"data/sunrgbd_nyu\"\n",
    "dev = DepthDataset(dev_txt, devDir, \n",
    "                     transform = transforms.Compose([ToFloat(),\n",
    "                                                     CenterCrop((320, 400)), \n",
    "                                                     NormalizeRGB(mean, var),\n",
    "                                                     ToTensor(),])\n",
    "#                      transform=transforms.Compose([ToFloat(), Crop_8(), ToTensor()])\n",
    "#                      transform=transforms.Compose([ToFloat(), Crop_small(), ToTensor()])\n",
    "                    )\n",
    "print(\"Loaded dev dataset from {} with size {}.\".format(dev_txt, len(dev)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded checkpoint 'checkpoints/checkpoint_epoch_49.pth.tar' (trained for 50 epoch(s)).\n",
      "loaded checkpointfile: checkpoints/checkpoint_epoch_49.pth.tar\n",
      "start_epoch: 50\n",
      "global_it: 0\n",
      "optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 1e-05\n",
      "    lr: 1e-05\n",
      "    weight_decay: 0\n",
      ")\n",
      "batch_size: 10\n",
      "num_epochs: 50\n",
      "learning rate (initial): 1e-05\n",
      "scheduler: {'milestones': [], 'gamma': 0.1, 'base_lrs': [1e-05], 'last_epoch': -1}\n"
     ]
    }
   ],
   "source": [
    "# Set up training.\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "checkpointfile = \"checkpoints/checkpoint_epoch_49.pth.tar\"\n",
    "# lam = 1e-8 # Weight decay parameter for L2 regularization\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 50\n",
    "batch_size = 10\n",
    "val_batch_size = 5\n",
    "\n",
    "# Build model and loss\n",
    "# Hyperparameters\n",
    "input_nc = 3\n",
    "output_nc = 1\n",
    "\n",
    "model = DepthNet(input_nc, output_nc)\n",
    "\n",
    "# Tensorboardx\n",
    "writer = SummaryWriter()\n",
    "# data_trainloss = \"data/trainloss\"\n",
    "# data_valloss = \"data/valloss\"\n",
    "# Image = \"Image\"\n",
    "\n",
    "#################\n",
    "# Loss function #\n",
    "#################\n",
    "\n",
    "def berhu_loss(prediction, target):\n",
    "    diff = prediction - target\n",
    "    threshold = 0.2*torch.max(torch.abs(prediction - target))\n",
    "    c = threshold.detach()\n",
    "    l2_part = torch.sum((diff**2 + c**2))/(2*c)\n",
    "    l1_part = torch.sum(torch.abs(diff))\n",
    "    return l1_part+l2_part\n",
    "\n",
    "# loss = nn.SmoothL1Loss()\n",
    "# loss = berhu_loss\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    loss.cuda()\n",
    "\n",
    "# Checkpointing\n",
    "if checkpointfile is not None:\n",
    "    if torch.cuda.is_available():\n",
    "        checkpoint = torch.load(checkpointfile)\n",
    "    else:\n",
    "        # Load GPU model on CPU\n",
    "        checkpoint = torch.load(checkpointfile,\n",
    "                                map_location=lambda storage,\n",
    "                                loc: storage)\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_loss = checkpoint['best_loss']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer.load_state_dict(checkpoint['optim_state_dict'])\n",
    "    trainlosses = checkpoint['trainlosses']\n",
    "    for i, trainloss in enumerate(trainlosses): # For tensorboardx\n",
    "        writer.add_scalar(\"data/trainloss\", trainloss, i)\n",
    "    vallosses = checkpoint['vallosses']\n",
    "    for i, valloss in enumerate(vallosses): # For tensorboardx\n",
    "        writer.add_scalar(\"data/valloss\", valloss, i)\n",
    "    global_it = len(trainlosses)\n",
    "    print(\"=> loaded checkpoint '{}' (trained for {} epoch(s)).\".format(checkpointfile, start_epoch))\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    global_it = 0 # Track global iterations\n",
    "    best_loss = torch.FloatTensor([float('inf')])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    trainlosses = []\n",
    "    vallosses = []\n",
    "    # Initialize weights:\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"conv\" in name and \"weight\" in name:\n",
    "#             print(name)\n",
    "            nn.init.xavier_normal_(param)\n",
    "        if \"norm\" in name and \"weight\" in name:\n",
    "#             print(name)\n",
    "            nn.init.constant_(param, 1)\n",
    "        elif \"bias\" in name:\n",
    "            nn.init.constant_(param, 0)\n",
    "            \n",
    "# Scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[], gamma=0.1)\n",
    "\n",
    "# Print summary of setup:\n",
    "print(\"loaded checkpointfile: {}\".format(checkpointfile))\n",
    "print(\"start_epoch: {}\".format(start_epoch))\n",
    "print(\"global_it: {}\".format(global_it))\n",
    "print(\"optimizer: {}\".format(optimizer))\n",
    "print(\"batch_size: {}\".format(batch_size))\n",
    "print(\"num_epochs: {}\".format(num_epochs))\n",
    "print(\"learning rate (initial): {}\".format(learning_rate))\n",
    "print(\"scheduler: {}\".format(scheduler.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50\n",
      "\titeration: 0\ttrain loss: 0.46216171979904175\n",
      "\titeration: 10\ttrain loss: 0.2399614304304123\n",
      "\titeration: 20\ttrain loss: 0.5442972779273987\n",
      "\titeration: 30\ttrain loss: 0.288849800825119\n",
      "\titeration: 40\ttrain loss: 0.4346057176589966\n",
      "\titeration: 50\ttrain loss: 0.30571889877319336\n",
      "\titeration: 60\ttrain loss: 0.36826014518737793\n",
      "\titeration: 70\ttrain loss: 0.3332650363445282\n",
      "\titeration: 80\ttrain loss: 0.2840162515640259\n",
      "\titeration: 90\ttrain loss: 0.4139283299446106\n",
      "\titeration: 100\ttrain loss: 0.32223719358444214\n",
      "\titeration: 110\ttrain loss: 0.5264000296592712\n",
      "End epoch 50\tval loss: 1.5325142062943558\n",
      "=> Saving checkpoint to: checkpoints/checkpoint_epoch_50.pth.tar\n",
      "epoch: 51\n",
      "\titeration: 0\ttrain loss: 0.4966995120048523\n",
      "\titeration: 10\ttrain loss: 0.4325902760028839\n",
      "\titeration: 20\ttrain loss: 0.3255394399166107\n",
      "\titeration: 30\ttrain loss: 0.8746808767318726\n",
      "\titeration: 40\ttrain loss: 0.554697573184967\n",
      "\titeration: 50\ttrain loss: 0.40921124815940857\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# Run the training #\n",
    "####################\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(dev, batch_size=val_batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "    print(\"epoch: {}\".format(epoch))\n",
    "    data = None\n",
    "    output = None\n",
    "    for it, data in enumerate(train_loader):\n",
    "        depth = data[\"depth\"].float()\n",
    "        rgb = data[\"rgb\"].float()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            depth = depth.cuda()\n",
    "            rgb = rgb.cuda()\n",
    "        # New batch\n",
    "#         print(rgb.dtype)\n",
    "        scheduler.optimizer.zero_grad()\n",
    "        output = model(rgb)\n",
    "        \n",
    "        # Save the first batch output of every epoch\n",
    "        \n",
    "#         a = list(model.parameters())[0].clone()\n",
    "\n",
    "        trainloss = loss(output, depth)\n",
    "        trainloss.backward()\n",
    "#         print(list(model.parameters())[0].grad)\n",
    "        scheduler.optimizer.step()\n",
    "#         print(depth)\n",
    "#         print(output)\n",
    "#         b = list(model.parameters())[0].clone()\n",
    "\n",
    "        if not (it % 10):\n",
    "            print(\"\\titeration: {}\\ttrain loss: {}\".format(it, trainloss.item()))\n",
    "#         trainlosses.append(trainloss.item())\n",
    "        writer.add_scalar(\"data/trainloss\", trainloss.item(), global_it)\n",
    "        \n",
    "        # TESTING:\n",
    "#         if not ((it + 1) % 5):\n",
    "#             # Stop after 5 batches\n",
    "#             break\n",
    "\n",
    "#         print(torch.equal(a.data, b.data))\n",
    "        global_it += 1\n",
    "    # Checkpointing\n",
    "    # Get bool not ByteTensor\"\n",
    "    valloss = validate(loss, model, val_loader)\n",
    "    print(\"End epoch {}\\tval loss: {}\".format(epoch, valloss))\n",
    "#     vallosses.append(valloss)\n",
    "    writer.add_scalar(\"data/valloss\", valloss, epoch)\n",
    "\n",
    "    # Save the last batch output of every epoch\n",
    "    rgb_input = vutils.make_grid(data[\"rgb\"], nrow=batch_size, normalize=True, scale_each=True)\n",
    "    writer.add_image('image/rgb_input', rgb_input, epoch)\n",
    "    \n",
    "    depth_truth = vutils.make_grid(data[\"depth\"], nrow=batch_size, normalize=True, scale_each=True)\n",
    "    writer.add_image('image/depth_truth', depth_truth, epoch)\n",
    "    \n",
    "    depth_output = vutils.make_grid(output, nrow=batch_size, normalize=True, scale_each=True)\n",
    "    writer.add_image('image/depth_output', depth_output, epoch)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        writer.add_histogram(name, param.clone().cpu().data.numpy(), global_it)\n",
    "#     save_images(data[\"rgb\"], data[\"depth\"], output, outputDir=\"images\", filename=\"epoch_{}\".format(epoch))\n",
    "    \n",
    "    is_best = bool(trainloss.data.cpu().numpy() < best_loss.numpy())\n",
    "    # Get greater Tensor to keep track best acc\n",
    "    best_loss = torch.FloatTensor(min(trainloss.data.cpu().numpy(), best_loss.numpy()))\n",
    "    # Save checkpoint\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_loss': best_loss,\n",
    "        'optim_state_dict': optimizer.state_dict(),\n",
    "        'trainlosses': trainlosses,\n",
    "        'vallosses': vallosses\n",
    "    }, is_best, filename=\"checkpoints/checkpoint_epoch_{}.pth.tar\".format(epoch), always_save=True)\n",
    "\n",
    "# Close tensorboardX    \n",
    "# writer.export_scalars_to_json(\"./all_scalars.json\") # for other processing\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize training loss\n",
    "checkpoint = torch.load(\"checkpoints/checkpoint_epoch_29.pth.tar\")\n",
    "trainlosses = checkpoint['trainlosses']\n",
    "plt.semilogy(trainlosses)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training loss\n",
    "\n",
    "trainlosses = checkpoint['vallosses']\n",
    "plt.semilogy(trainlosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import imshow\n",
    "print(type(rgb))\n",
    "imshow(rgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# Load data\n",
    "a = torch.ones(1, requires_grad=True)\n",
    "b = torch.ones(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + b\n",
    "c.register_hook(print)\n",
    "s = a**2 + b**2\n",
    "s.register_hook(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e = torch.sum(c)\n",
    "e = c.detach()\n",
    "f = torch.sum(s)\n",
    "g = f/e\n",
    "g.backward()\n",
    "# s.backward()\n",
    "# f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
