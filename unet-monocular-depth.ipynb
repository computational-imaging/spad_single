{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net Implementation of Monocular Depth Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda enabled on device: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import functools\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Set Device\n",
    "    torch.cuda.set_device(0)\n",
    "    print(\"Cuda enabled on device: {}\".format(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile depthnet.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from collections import OrderedDict\n",
    "\n",
    "def create_sublayer_dict(input_nc, output_nc, layer_index, nconvs, norm_layer, **conv_kwargs):\n",
    "    sublayer=OrderedDict()\n",
    "    j = 0\n",
    "    sublayer.update({\"conv{}_{}\".format(layer_index, j): nn.Conv2d(input_nc, output_nc, **conv_kwargs)})\n",
    "    j += 1\n",
    "    sublayer.update({\"relu{}_{}\".format(layer_index, j): nn.ReLU(True)})\n",
    "    for i in range(nconvs-1):\n",
    "        j += 1\n",
    "        sublayer.update({\"conv{}_{}\".format(layer_index, j): nn.Conv2d(output_nc, output_nc, **conv_kwargs)})\n",
    "        j += 1\n",
    "        sublayer.update({\"relu{}_{}\".format(layer_index, j): nn.ReLU(True)})\n",
    "    if norm_layer:\n",
    "        j += 1\n",
    "        sublayer.update({\"norm{}_{}\".format(layer_index, j): norm_layer(output_nc)})\n",
    "    return sublayer\n",
    "\n",
    "class DepthNet(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, norm_layer=nn.BatchNorm2d):\n",
    "        super(DepthNet, self).__init__()\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        use_bias = True\n",
    "\n",
    "        # Conv1\n",
    "#         model1=OrderedDict()\n",
    "#         model1.update({\"conv1_0\": nn.Conv2d(input_nc, 64, kernel_size=3, stride=1, padding=1, bias=use_bias)})\n",
    "#         model1.update({\"relu1_1\": nn.ReLU(True)})\n",
    "#         model1.update({\"conv1_2\": nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=use_bias)})\n",
    "#         model1.update({\"relu1_3\": nn.ReLU(True)})\n",
    "#         model1.update({\"norm1_4\": norm_layer(64)})\n",
    "        model1 = create_sublayer_dict(input_nc, 64, 1, 2, norm_layer, \n",
    "                                      kernel_size=3, stride=1, padding=1, bias=use_bias)\n",
    "        # add a subsampling operation (in self.forward())\n",
    "\n",
    "        # Conv2\n",
    "#         model2=OrderedDict()\n",
    "#         model2.update({\"conv2_0\": nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=use_bias)})\n",
    "#         model2.update({\"relu2_1\": nn.ReLU(True)})\n",
    "#         model2.update({\"conv2_2\": nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias)})\n",
    "#         model2.update({\"relu2_3\": nn.ReLU(True)})\n",
    "#         model2.update({\"norm2_4\": norm_layer(128)})\n",
    "        model2 = create_sublayer_dict(64, 128, 2, 2, norm_layer,\n",
    "                                      kernel_size=3, stride=1, padding=1, bias=use_bias)\n",
    "        # add a subsampling layer operation (in self.forward())\n",
    "\n",
    "        # Conv3\n",
    "#         model3=OrderedDict()\n",
    "#         model3.update({\"conv3_0\": nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=use_bias)})\n",
    "#         model3.update({\"relu3_1\": nn.ReLU(True)})\n",
    "#         model3.update({\"conv3_2\": nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias)})\n",
    "#         model3.update({\"relu3_3\": nn.ReLU(True)})\n",
    "#         model3.update({\"conv3_4\": nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias)})\n",
    "#         model3.update({\"relu3_5\": nn.ReLU(True)})\n",
    "#         model3.update({\"norm3_6\": norm_layer(256)})\n",
    "        model3 = create_sublayer_dict(128, 256, 3, 3, norm_layer,\n",
    "                                      kernel_size=3, stride=1, padding=1, bias=use_bias)\n",
    "        # add a subsampling layer operation\n",
    "\n",
    "        # Conv4\n",
    "#         model4=OrderedDict()\n",
    "#         model4.update({\"conv4_0\": nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=use_bias)})\n",
    "#         model4.update({\"relu4_1\": nn.ReLU(True)})\n",
    "#         model4.update({\"conv4_2\": nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias)})\n",
    "#         model4.update({\"relu4_3\": nn.ReLU(True)})\n",
    "#         model4.update({\"conv4_4\": nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias)})\n",
    "#         model4.update({\"relu4_5\": nn.ReLU(True)})\n",
    "#         model4.update({\"norm4_6\": norm_layer(512)})\n",
    "        model4 = create_sublayer_dict(256, 512, 4, 3, norm_layer,\n",
    "                                      kernel_size=3, stride=1, padding=1, bias=use_bias)\n",
    "\n",
    "        # Conv5\n",
    "#         model5=OrderedDict()\n",
    "#         model5.update({\"conv5_0\":nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias)})\n",
    "#         model5+=[nn.ReLU(True),]\n",
    "#         model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "#         model5+=[nn.ReLU(True),]\n",
    "#         model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "#         model5+=[nn.ReLU(True),]\n",
    "#         model5+=[norm_layer(512),]\n",
    "        model5 = create_sublayer_dict(512, 512, 5, 3, norm_layer,\n",
    "                                      kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias)\n",
    "\n",
    "        # Conv6\n",
    "#         model6=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "#         model6+=[nn.ReLU(True),]\n",
    "#         model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "#         model6+=[nn.ReLU(True),]\n",
    "#         model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "#         model6+=[nn.ReLU(True),]\n",
    "#         model6+=[norm_layer(512),]\n",
    "        model6 = create_sublayer_dict(512, 512, 6, 3, norm_layer,\n",
    "                                      kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias)\n",
    "\n",
    "        # Conv7\n",
    "#         model7=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "#         model7+=[nn.ReLU(True),]\n",
    "#         model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "#         model7+=[nn.ReLU(True),]\n",
    "#         model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "#         model7+=[nn.ReLU(True),]\n",
    "#         model7+=[norm_layer(512),]\n",
    "        model7 = create_sublayer_dict(512, 512, 7, 3, norm_layer,\n",
    "                                      kernel_size=3, stride=1, padding=1, bias=use_bias)\n",
    "\n",
    "        # Conv7\n",
    "        model8up=OrderedDict([(\"convt8_up\", \n",
    "                               nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=use_bias))]\n",
    "                            )\n",
    "        model3short8=OrderedDict([(\"conv3short8\",\n",
    "                                   nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias))]\n",
    "                                )\n",
    "\n",
    "        model8=OrderedDict([(\"relu8_pre\", nn.ReLU(True))])\n",
    "#         model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "#         model8+=[nn.ReLU(True),]\n",
    "#         model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "#         model8+=[nn.ReLU(True),]\n",
    "#         model8+=[norm_layer(256),]\n",
    "        model8.update(create_sublayer_dict(256, 256, 8, 2, norm_layer,\n",
    "                                      kernel_size=3, stride=1, padding=1, bias=use_bias))\n",
    "\n",
    "        # Conv9\n",
    "        model9up=OrderedDict([(\"convt9_up\", \n",
    "                               nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=use_bias))]\n",
    "                            )\n",
    "        model2short9=OrderedDict([(\"conv2short9\",\n",
    "                                   nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias))]\n",
    "                                )\n",
    "        # add the two feature maps above        \n",
    "\n",
    "        model9=OrderedDict([(\"relu9_pre\", nn.ReLU(True))])\n",
    "#         model9+=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "#         model9+=[nn.ReLU(True),]\n",
    "#         model9+=[norm_layer(128),]\n",
    "        model9.update(create_sublayer_dict(128, 128, 9, 1, norm_layer,\n",
    "                                           kernel_size=3, stride=1, padding=1, bias=use_bias))\n",
    "\n",
    "        # Conv10\n",
    "        model10up=OrderedDict([(\"conv10_up\", \n",
    "                               nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1, bias=use_bias))]\n",
    "                             )\n",
    "        model1short10=OrderedDict([(\"conv1short10\", \n",
    "                                    nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=use_bias))]\n",
    "                                 )\n",
    "        # add the two feature maps above\n",
    "\n",
    "        model10=OrderedDict([(\"relu10_pre\", nn.ReLU(True))])\n",
    "        model10.update({\"conv10_0\": nn.Conv2d(128, 128, kernel_size=3, dilation=1, stride=1, padding=1, bias=use_bias)})\n",
    "        model10.update({\"leakyrelu10_1\": nn.LeakyReLU(negative_slope=.2)})\n",
    "\n",
    "        # Depth Map Regression Output\n",
    "        model_out=OrderedDict([(\"conv_out\",\n",
    "                                nn.Conv2d(128, 1, kernel_size=1, padding=0, dilation=1, stride=1, bias=use_bias))]\n",
    "                             )\n",
    "        model_out.update({\"relu_out\": nn.ReLU(True)}) # Depth should be in [0, +inf)\n",
    "\n",
    "        self.model1 = nn.Sequential(model1)\n",
    "        self.model2 = nn.Sequential(model2)\n",
    "        self.model3 = nn.Sequential(model3)\n",
    "        self.model4 = nn.Sequential(model4)\n",
    "        self.model5 = nn.Sequential(model5)\n",
    "        self.model6 = nn.Sequential(model6)\n",
    "        self.model7 = nn.Sequential(model7)\n",
    "        self.model8up = nn.Sequential(model8up)\n",
    "        self.model8 = nn.Sequential(model8)\n",
    "        self.model9up = nn.Sequential(model9up)\n",
    "        self.model9 = nn.Sequential(model9)\n",
    "        self.model10up = nn.Sequential(model10up)\n",
    "        self.model10 = nn.Sequential(model10)\n",
    "        self.model3short8 = nn.Sequential(model3short8)\n",
    "        self.model2short9 = nn.Sequential(model2short9)\n",
    "        self.model1short10 = nn.Sequential(model1short10)\n",
    "\n",
    "        self.model_out = nn.Sequential(model_out)\n",
    "\n",
    "#         self.upsample4 = nn.Sequential(*[nn.Upsample(scale_factor=4, mode='nearest'),])\n",
    "#         self.softmax = nn.Sequential(*[nn.Softmax(dim=1),])\n",
    "\n",
    "    def forward(self, input_A):\n",
    "#         conv1_2 = self.model1(torch.cat((input_A,input_B,mask_B),dim=1))\n",
    "        conv1_2 = self.model1(input_A)\n",
    "        conv2_2 = self.model2(conv1_2[:,:,::2,::2]) # downsample\n",
    "        conv3_3 = self.model3(conv2_2[:,:,::2,::2]) # downsample\n",
    "        conv4_3 = self.model4(conv3_3[:,:,::2,::2]) # downsample\n",
    "        conv5_3 = self.model5(conv4_3)\n",
    "        conv6_3 = self.model6(conv5_3)\n",
    "        conv7_3 = self.model7(conv6_3)\n",
    "        conv8_up = self.model8up(conv7_3) + self.model3short8(conv3_3) # Shortcut\n",
    "        conv8_3 = self.model8(conv8_up)\n",
    "        conv9_up = self.model9up(conv8_3) + self.model2short9(conv2_2) # Shortcut\n",
    "        conv9_3 = self.model9(conv9_up)\n",
    "        conv10_up = self.model10up(conv9_3) + self.model1short10(conv1_2) # Shortcut\n",
    "        conv10_2 = self.model10(conv10_up)\n",
    "        out_reg = self.model_out(conv10_2)\n",
    "\n",
    "        return out_reg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# Dataset #\n",
    "###########\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import csv, numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "class DepthDataset(Dataset):\n",
    "    \"\"\"Class for reading and storing image and depth data together.\n",
    "    \"\"\"\n",
    "    def __init__(self, splitfile, dataDir, transform=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        images : list of (string, string)\n",
    "            list of (depth_map_path, rgb_path) filepaths to depth maps and their rgb images.\n",
    "        load_depth_map : function\n",
    "            the function for loading this particular kind of depth_map\n",
    "        load_rgb : function\n",
    "            the function for loading this particular kind of image.\n",
    "        \"\"\"\n",
    "        super(DepthDataset, self).__init__()\n",
    "        self.dataDir = dataDir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        with open(splitfile, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                self.data.append(line.strip().split(\",\"))\n",
    "#         print(self.data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        depthFile, rgbFile = self.data[idx]\n",
    "        # Extract depth file:\n",
    "        depthImg = Image.open(os.path.join(self.dataDir, depthFile))\n",
    "        # Extract rgb file:\n",
    "        rgbImg = Image.open(os.path.join(self.dataDir, rgbFile))\n",
    "        sample = {\"depth\": depthImg, \"rgb\": rgbImg}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Transforms #\n",
    "##############\n",
    "# for data augmentation\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        depth, rgb = sample['depth'], sample['rgb']\n",
    "\n",
    "        h, w = depth.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        depth = depth[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "        \n",
    "        rgb = rgb[top: top + new_h,\n",
    "                  left: left + new_w]\n",
    "\n",
    "        return {'depth': depth, 'rgb': rgb}\n",
    "\n",
    "class CenterCrop(object):\n",
    "    \"\"\"Center crop the image\n",
    "    \n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "            \n",
    "    def __call__(self, sample):\n",
    "        depth, rgb = sample['depth'], sample['rgb']\n",
    "        h, w = depth.shape\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = h//2 - new_h//2\n",
    "        bottom = h//2 + new_h//2 + (1 if new_h % 2 else 0)\n",
    "        left = w - new_w//2\n",
    "        right = w + new_w//2 + (1 if new_w % 2 else 0)\n",
    "        \n",
    "        return {\"depth\": depth[top:bottom, left:right],\n",
    "                \"rgb\": rgb[top:bottom, left:right, :]}\n",
    "    \n",
    "class Crop_8(object):\n",
    "    \"\"\"Crop to a size where both dimensions are divisible by 8\"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        depth, rgb = sample['depth'], sample['rgb']\n",
    "        new_h, new_w = (depth.shape[0]//8)*8, (depth.shape[1]//8)*8\n",
    "        return {\"depth\": depth[:new_h, :new_w],\n",
    "                \"rgb\": rgb[:new_h, :new_w, :]}\n",
    "        \n",
    "class Crop_small(object):\n",
    "    def __call__(self, sample):\n",
    "        depth, rgb = sample['depth'], sample['rgb']\n",
    "        h, w = depth.shape[:2]\n",
    "        x = 16\n",
    "        return {\"depth\": depth[h//2-x:h//2+x, w//2-x:w//2+x],\n",
    "                \"rgb\": rgb[h//2-x:h//2+x, w//2-x:w//2+x, :]}\n",
    "\n",
    "class ToFloat(object):\n",
    "    \"\"\"Also parses the depth info for sunrgbd.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        depth = sample['depth']\n",
    "        x = np.asarray(depth, dtype=np.int16)\n",
    "        y = (x >> 3) | (x << 16-3)\n",
    "        z = y.astype(np.float32)/1000\n",
    "        z[z>8.] = 8.\n",
    "        return {\"depth\": z,\n",
    "                \"rgb\": np.asarray(sample['rgb']).astype(np.float32)}\n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        depth, rgb = sample['depth'], sample['rgb']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "#         depth = depth.transpose((2, 0, 1))\n",
    "        rgb = rgb.transpose((2, 0, 1))\n",
    "        return {'depth': torch.from_numpy(depth)[np.newaxis,:,:],\n",
    "                'rgb': torch.from_numpy(rgb)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.cuda as cuda\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import os.path\n",
    "# Helper functions\n",
    "\n",
    "#################\n",
    "# Checkpointing #\n",
    "#################\n",
    "def save_checkpoint(state, is_best, filename='/output/checkpoint.pth.tar', always_save=False):\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    if is_best or always_save:\n",
    "        print (\"=> Saving checkpoint to: {}\".format(filename))\n",
    "        torch.save(state, filename)  # save checkpoint\n",
    "    else:\n",
    "        print (\"=> Validation Accuracy did not improve\")\n",
    "\n",
    "##############\n",
    "# Validation #\n",
    "##############\n",
    "def validate(loss, model, val_loader):\n",
    "    \"\"\"Computes the validation error of the model on the validation set.\n",
    "    val_loader should be a DataLoader.\n",
    "    \n",
    "    Returns an ordinary number (i.e. not a tensor)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    it = None\n",
    "    losses = []\n",
    "    for it, data in enumerate(val_loader):\n",
    "        depth = data[\"depth\"].float()\n",
    "        rgb = data[\"rgb\"].float()\n",
    "        if torch.cuda.is_available():\n",
    "            depth = depth.cuda()\n",
    "            rgb = rgb.cuda()\n",
    "        rgb.requires_grad=False\n",
    "        output = model(rgb)\n",
    "        losses.append(loss(output, depth).item())\n",
    "    nbatches = it+1\n",
    "    return sum(losses)/nbatches\n",
    "\n",
    "##################\n",
    "# Viewing Images #\n",
    "##################\n",
    "def save_images(*batches, outputDir, filename):\n",
    "    \"\"\"\n",
    "    Given a list of tensors of size (B, C, H, W) (batch, channels, height, width) torch.Tensor\n",
    "    Saves each entry of the batch as an rgb or grayscale image, depending on how many channels\n",
    "    the image has.\n",
    "    \"\"\"\n",
    "    I = None\n",
    "    trans = transforms.ToPILImage()\n",
    "    for batchnum, batch in enumerate(batches):\n",
    "        if batch.shape[1] == 3:\n",
    "            pass\n",
    "        elif batch.shape[1] == 1:\n",
    "            batch /= torch.max(batch) # normalize to lie in [0, 1]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported number of channels: {}\".format(batch.shape[1]))\n",
    "        batch = batch.type(torch.float32)\n",
    "        for img in range(batch.shape[0]):            \n",
    "            I = trans(batch[img,:,:,:].cpu().detach())\n",
    "            I.save(os.path.join(outputDir, filename + \"_{}_{}.png\".format(batchnum, img)))\n",
    "\n",
    "\n",
    "############\n",
    "# Plotting #\n",
    "############\n",
    "def save_train_val_loss_plots(trainlosses, vallosses, epoch):\n",
    "    # Train loss\n",
    "    fig = plt.figure()\n",
    "    plt.plot(trainlosses)\n",
    "    plt.title(\"Train loss\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(\"trainloss_epoch{}.png\".format(epoch))\n",
    "    # Train loss\n",
    "    fig = plt.figure()\n",
    "    plt.plot(trainlosses)\n",
    "    plt.title(\"Val loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(\"Val loss{}.png\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training dataset from data/sunrgbd_nyu/train.txt with size 1159.\n",
      "Loaded dev dataset from data/sunrgbd_nyu/dev.txt with size 145.\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train_txt = \"data/sunrgbd_nyu/train.txt\"\n",
    "trainDir = \"data/sunrgbd_nyu\"\n",
    "train = DepthDataset(train_txt, trainDir, \n",
    "                     transform = transforms.Compose([ToFloat(), RandomCrop((400, 320)), ToTensor()])\n",
    "#                      transform=transforms.Compose([ToFloat(), Crop_8(), ToFloat(), ToTensor()])\n",
    "#                      transform=transforms.Compose([ToFloat(), Crop_small(), ToFloat(), ToTensor()])\n",
    "                    )\n",
    "\n",
    "print(\"Loaded training dataset from {} with size {}.\".format(train_txt, len(train)))\n",
    "\n",
    "dev_txt = \"data/sunrgbd_nyu/dev.txt\"\n",
    "devDir = \"data/sunrgbd_nyu\"\n",
    "dev = DepthDataset(dev_txt, devDir, \n",
    "                     transform = transforms.Compose([ToFloat(), CenterCrop((400, 320)), ToTensor()])\n",
    "#                      transform=transforms.Compose([ToFloat(), Crop_8(), ToTensor()])\n",
    "#                      transform=transforms.Compose([ToFloat(), Crop_small(), ToTensor()])\n",
    "                    )\n",
    "print(\"Loaded dev dataset from {} with size {}.\".format(dev_txt, len(dev)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1.conv1_0.weight\n",
      "model1.conv1_2.weight\n",
      "model1.norm1_4.weight\n",
      "model2.conv2_0.weight\n",
      "model2.conv2_2.weight\n",
      "model2.norm2_4.weight\n",
      "model3.conv3_0.weight\n",
      "model3.conv3_2.weight\n",
      "model3.conv3_4.weight\n",
      "model3.norm3_6.weight\n",
      "model4.conv4_0.weight\n",
      "model4.conv4_2.weight\n",
      "model4.conv4_4.weight\n",
      "model4.norm4_6.weight\n",
      "model5.conv5_0.weight\n",
      "model5.conv5_2.weight\n",
      "model5.conv5_4.weight\n",
      "model5.norm5_6.weight\n",
      "model6.conv6_0.weight\n",
      "model6.conv6_2.weight\n",
      "model6.conv6_4.weight\n",
      "model6.norm6_6.weight\n",
      "model7.conv7_0.weight\n",
      "model7.conv7_2.weight\n",
      "model7.conv7_4.weight\n",
      "model7.norm7_6.weight\n",
      "model8up.convt8_up.weight\n",
      "model8.conv8_0.weight\n",
      "model8.conv8_2.weight\n",
      "model8.norm8_4.weight\n",
      "model9up.convt9_up.weight\n",
      "model9.conv9_0.weight\n",
      "model9.norm9_2.weight\n",
      "model10up.conv10_up.weight\n",
      "model10.conv10_0.weight\n",
      "model3short8.conv3short8.weight\n",
      "model2short9.conv2short9.weight\n",
      "model1short10.conv1short10.weight\n",
      "model_out.conv_out.weight\n",
      "loaded checkpointfile: None\n",
      "start_epoch: 0\n",
      "optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "batch_size: 10\n",
      "num_epochs: 30\n",
      "learning rate (initial): 0.001\n"
     ]
    }
   ],
   "source": [
    "# Set up training.\n",
    "import torch.optim as optim\n",
    "\n",
    "checkpointfile = None\n",
    "# lam = 1e-8 # Weight decay parameter for L2 regularization\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 30\n",
    "batch_size = 10\n",
    "\n",
    "# Build model and loss\n",
    "# Hyperparameters\n",
    "input_nc = 3\n",
    "output_nc = 1\n",
    "\n",
    "model = DepthNet(input_nc, output_nc)\n",
    "\n",
    "#################\n",
    "# Loss function #\n",
    "#################\n",
    "\n",
    "def berhu_loss(prediction, target):\n",
    "    diff = prediction - target\n",
    "    threshold = 0.2*torch.max(torch.abs(prediction - target))\n",
    "    c = threshold.detach()\n",
    "    l2_part = torch.sum((diff**2 + c**2))/(2*c)\n",
    "    l1_part = torch.sum(torch.abs(diff))\n",
    "    return l1_part+l2_part\n",
    "\n",
    "# loss = nn.SmoothL1Loss()\n",
    "# loss = berhu_loss\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    loss.cuda()\n",
    "\n",
    "# Checkpointing\n",
    "if checkpointfile is not None:\n",
    "    if torch.cuda.is_available():\n",
    "        checkpoint = torch.load(checkpointfile)\n",
    "    else:\n",
    "        # Load GPU model on CPU\n",
    "        checkpoint = torch.load(checkpointfile,\n",
    "                                map_location=lambda storage,\n",
    "                                loc: storage)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_loss = checkpoint['best_loss']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer.load_state_dict(checkpoint['optim_state_dict'])\n",
    "    trainlosses = checkpoint['trainlosses']\n",
    "#     vallosses = checkpoint['vallosses']\n",
    "    print(\"=> loaded checkpoint '{}' (trained for {} epochs)\".format(resume_weights, checkpoint['epoch']))\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    best_loss = torch.FloatTensor([float('inf')])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    trainlosses = []\n",
    "    vallosses = []\n",
    "    # Initialize weights:\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"conv\" in name and \"weight\" in name:\n",
    "            print(name)\n",
    "            nn.init.xavier_normal_(param)\n",
    "        if \"norm\" in name and \"weight\" in name:\n",
    "            print(name)\n",
    "            nn.init.constant_(param, 1)\n",
    "        elif \"bias\" in name:\n",
    "            nn.init.constant_(param, 0)\n",
    "            \n",
    "# Scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5, 10, 20], gamma=0.1)\n",
    "\n",
    "# Print summary of setup:\n",
    "print(\"loaded checkpointfile: {}\".format(checkpointfile))\n",
    "print(\"start_epoch: {}\".format(start_epoch))\n",
    "print(\"optimizer: {}\".format(optimizer))\n",
    "print(\"batch_size: {}\".format(batch_size))\n",
    "print(\"num_epochs: {}\".format(num_epochs))\n",
    "print(\"learning rate (initial): {}\".format(learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "\titeration: 0\ttrain loss: 6.660920143127441\n",
      "\titeration: 10\ttrain loss: 5.001108646392822\n",
      "\titeration: 20\ttrain loss: 4.368201732635498\n",
      "\titeration: 30\ttrain loss: 4.636993885040283\n",
      "\titeration: 40\ttrain loss: 4.026307582855225\n",
      "\titeration: 50\ttrain loss: 4.151325702667236\n",
      "\titeration: 60\ttrain loss: 2.713653564453125\n",
      "\titeration: 70\ttrain loss: 7.817407131195068\n",
      "\titeration: 80\ttrain loss: 7.717471122741699\n",
      "\titeration: 90\ttrain loss: 7.311159610748291\n",
      "\titeration: 100\ttrain loss: 5.854341983795166\n",
      "\titeration: 110\ttrain loss: 3.552532911300659\n",
      "End epoch 0\tval loss: 4.129573836408812\n",
      "=> Saving checkpoint to: checkpoints/checkpoint_epoch_0.pth.tar\n",
      "epoch: 1\n",
      "\titeration: 0\ttrain loss: 3.346029758453369\n",
      "\titeration: 10\ttrain loss: 4.425172328948975\n",
      "\titeration: 20\ttrain loss: 3.7099063396453857\n",
      "\titeration: 30\ttrain loss: 4.798088073730469\n",
      "\titeration: 40\ttrain loss: 4.025942325592041\n",
      "\titeration: 50\ttrain loss: 6.871330261230469\n",
      "\titeration: 60\ttrain loss: 4.025509357452393\n",
      "\titeration: 70\ttrain loss: 5.019134521484375\n",
      "\titeration: 80\ttrain loss: 4.856333255767822\n",
      "\titeration: 90\ttrain loss: 5.407094955444336\n",
      "\titeration: 100\ttrain loss: 3.094499111175537\n",
      "\titeration: 110\ttrain loss: 2.980513334274292\n",
      "End epoch 1\tval loss: 3.961402017494728\n",
      "=> Saving checkpoint to: checkpoints/checkpoint_epoch_1.pth.tar\n",
      "epoch: 2\n",
      "\titeration: 0\ttrain loss: 3.5184223651885986\n",
      "\titeration: 10\ttrain loss: 4.299950122833252\n",
      "\titeration: 20\ttrain loss: 5.570438861846924\n",
      "\titeration: 30\ttrain loss: 5.981224536895752\n",
      "\titeration: 40\ttrain loss: 2.164433479309082\n",
      "\titeration: 50\ttrain loss: 5.020866870880127\n",
      "\titeration: 60\ttrain loss: 3.0832154750823975\n",
      "\titeration: 70\ttrain loss: 3.827157735824585\n",
      "\titeration: 80\ttrain loss: 4.2246809005737305\n",
      "\titeration: 90\ttrain loss: 3.7954750061035156\n",
      "\titeration: 100\ttrain loss: 5.472465991973877\n",
      "\titeration: 110\ttrain loss: 3.143998146057129\n",
      "End epoch 2\tval loss: 3.9133190533210493\n",
      "=> Saving checkpoint to: checkpoints/checkpoint_epoch_2.pth.tar\n",
      "epoch: 3\n",
      "\titeration: 0\ttrain loss: 4.254985332489014\n",
      "\titeration: 10\ttrain loss: 3.676126480102539\n",
      "\titeration: 20\ttrain loss: 5.090383529663086\n",
      "\titeration: 30\ttrain loss: 5.6534576416015625\n",
      "\titeration: 40\ttrain loss: 6.3095598220825195\n",
      "\titeration: 50\ttrain loss: 2.424283266067505\n",
      "\titeration: 60\ttrain loss: 3.7251954078674316\n",
      "\titeration: 70\ttrain loss: 3.192074775695801\n",
      "\titeration: 80\ttrain loss: 5.033029556274414\n",
      "\titeration: 90\ttrain loss: 3.517352819442749\n",
      "\titeration: 100\ttrain loss: 3.729282855987549\n",
      "\titeration: 110\ttrain loss: 5.552684783935547\n",
      "End epoch 3\tval loss: 3.961052524632421\n",
      "=> Saving checkpoint to: checkpoints/checkpoint_epoch_3.pth.tar\n",
      "epoch: 4\n",
      "\titeration: 0\ttrain loss: 6.055174827575684\n",
      "\titeration: 10\ttrain loss: 2.9574713706970215\n",
      "\titeration: 20\ttrain loss: 4.571086883544922\n",
      "\titeration: 30\ttrain loss: 4.304720401763916\n",
      "\titeration: 40\ttrain loss: 2.525667428970337\n",
      "\titeration: 50\ttrain loss: 2.7460594177246094\n",
      "\titeration: 60\ttrain loss: 2.5081896781921387\n",
      "\titeration: 70\ttrain loss: 4.809781074523926\n",
      "\titeration: 80\ttrain loss: 3.48091721534729\n",
      "\titeration: 90\ttrain loss: 6.799954891204834\n",
      "\titeration: 100\ttrain loss: 4.391403675079346\n",
      "\titeration: 110\ttrain loss: 4.976490020751953\n",
      "End epoch 4\tval loss: 3.9212175535744636\n",
      "=> Saving checkpoint to: checkpoints/checkpoint_epoch_4.pth.tar\n",
      "epoch: 5\n",
      "\titeration: 0\ttrain loss: 3.167602062225342\n",
      "\titeration: 10\ttrain loss: 4.900933742523193\n",
      "\titeration: 20\ttrain loss: 3.342240333557129\n",
      "\titeration: 30\ttrain loss: 4.416103839874268\n",
      "\titeration: 40\ttrain loss: 6.892821788787842\n",
      "\titeration: 50\ttrain loss: 4.2475409507751465\n",
      "\titeration: 60\ttrain loss: 4.281147480010986\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# Run the training #\n",
    "####################\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(dev, batch_size=5, shuffle=True, num_workers=1)\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    print(\"epoch: {}\".format(epoch))\n",
    "    data = None\n",
    "    output = None\n",
    "    for it, data in enumerate(train_loader):\n",
    "        depth = data[\"depth\"].float()\n",
    "        rgb = data[\"rgb\"].float()\n",
    "#         print('ok')\n",
    "\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            depth = depth.cuda()\n",
    "            rgb = rgb.cuda()\n",
    "        # New batch\n",
    "#         print(rgb.dtype)\n",
    "        scheduler.optimizer.zero_grad()\n",
    "        output = model(rgb)\n",
    "        \n",
    "        # Save the first batch output of every epoch\n",
    "        \n",
    "#         a = list(model.parameters())[0].clone()\n",
    "\n",
    "        trainloss = loss(output, depth)\n",
    "        trainloss.backward()\n",
    "#         print(list(model.parameters())[0].grad)\n",
    "        scheduler.optimizer.step()\n",
    "#         print(depth)\n",
    "#         print(output)\n",
    "#         b = list(model.parameters())[0].clone()\n",
    "\n",
    "        if not (it % 10):\n",
    "            print(\"\\titeration: {}\\ttrain loss: {}\".format(it, trainloss.item()))\n",
    "        trainlosses.append(trainloss.item())\n",
    "        \n",
    "        # TESTING:\n",
    "#         if not ((it + 1) % 5):\n",
    "#             # Stop after 5 batches\n",
    "#             break\n",
    "\n",
    "#         print(torch.equal(a.data, b.data))\n",
    "    # Checkpointing\n",
    "    # Get bool not ByteTensor\"\n",
    "    valloss = validate(loss, model, val_loader)\n",
    "    print(\"End epoch {}\\tval loss: {}\".format(epoch, valloss))\n",
    "    vallosses.append(valloss)\n",
    "\n",
    "    # Save the last batch output of every epoch\n",
    "    save_images(data[\"rgb\"], data[\"depth\"], output, outputDir=\"images\", filename=\"epoch_{}\".format(epoch))\n",
    "    \n",
    "    is_best = bool(trainloss.data.cpu().numpy() < best_loss.numpy())\n",
    "    # Get greater Tensor to keep track best acc\n",
    "    best_loss = torch.FloatTensor(min(trainloss.data.cpu().numpy(), best_loss.numpy()))\n",
    "    # Save checkpoint\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_loss': best_loss,\n",
    "        'optim_state_dict': optimizer.state_dict(),\n",
    "        'trainlosses': trainlosses,\n",
    "        'vallosses': vallosses\n",
    "    }, is_best, filename=\"checkpoints/checkpoint_epoch_{}.pth.tar\".format(epoch), always_save=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize training loss\n",
    "checkpoint = torch.load(\"checkpoints/checkpoint_epoch_1499.pth.tar\")\n",
    "trainlosses = checkpoint['trainlosses']\n",
    "plt.semilogy(trainlosses)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import imshow\n",
    "print(type(rgb))\n",
    "imshow(rgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# Load data\n",
    "a = torch.ones(1, requires_grad=True)\n",
    "b = torch.ones(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + b\n",
    "c.register_hook(print)\n",
    "s = a**2 + b**2\n",
    "s.register_hook(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e = torch.sum(c)\n",
    "e = c.detach()\n",
    "f = torch.sum(s)\n",
    "g = f/e\n",
    "g.backward()\n",
    "# s.backward()\n",
    "# f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
