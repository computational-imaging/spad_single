{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda enabled on device: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import functools\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Set Device\n",
    "    torch.cuda.set_device(1)\n",
    "    print(\"Cuda enabled on device: {}\".format(torch.cuda.current_device()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from depthnet.model import DepthNet\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class DepthNetWithHints(nn.Module):\n",
    "    def __init__(self, depthnet, hist_len, num_hints_layers):\n",
    "        \"\"\"Takes an existing DepthNet, along with the size of the histogram and the size of its bins\"\"\"\n",
    "        super(DepthNetWithHints, self).__init__()\n",
    "        self.input_nc = depthnet.input_nc\n",
    "        self.output_nc = depthnet.output_nc\n",
    "        self.depthnet = depthnet\n",
    "        self.hist_len = hist_len\n",
    "        self.num_hints_layers = num_hints_layers\n",
    "        \n",
    "        # Create hints network\n",
    "        assert num_hints_layers > 0\n",
    "        # Extract number of out channels of conv4\n",
    "        hints_output = depthnet.model4[0].out_channels\n",
    "        hints = OrderedDict([(\"hints_conv_0\", nn.Conv2d(self.hist_len, hints_output, kernel_size=1))])\n",
    "        hints.update({\"hints_relu0_1\": nn.ReLU(True)})\n",
    "        j = 2\n",
    "        for _ in range(num_hints_layers-1):\n",
    "            hints.update({\"hints_conv_{}\".format(j): nn.Conv2d(hints_output, hints_output, kernel_size=1)})\n",
    "            j += 1\n",
    "            hints.update({\"hints_relu_{}\".format(j): nn.ReLU(True)})\n",
    "            j += 1\n",
    "        \n",
    "        self.global_hints = nn.Sequential(hints)\n",
    "        \n",
    "    def forward(self, input_A, hist):\n",
    "        # |hist| should be a (1, hist_len, 1, 1) tensor\n",
    "        # First 4 layers of regular depthnet\n",
    "        conv1_2 = self.depthnet.model1(input_A)\n",
    "        conv2_2 = self.depthnet.model2(conv1_2[:,:,::2,::2]) # downsample\n",
    "        conv3_3 = self.depthnet.model3(conv2_2[:,:,::2,::2]) # downsample\n",
    "        conv4_3 = self.depthnet.model4(conv3_3[:,:,::2,::2]) # downsample\n",
    "        \n",
    "        # Global hints network\n",
    "        hints_out = self.global_hints(hist)\n",
    "        # Replicate and add to output of conv4 (broadcasting takes care of this)\n",
    "        conv5_3 = self.depthnet.model5(conv4_3 + hints_out)\n",
    "        \n",
    "        # Finish doing the rest of the depthnet\n",
    "        conv6_3 = self.depthnet.model6(conv5_3)\n",
    "        conv7_3 = self.depthnet.model7(conv6_3)\n",
    "        conv8_up = self.depthnet.model8up(conv7_3) + self.depthnet.model3short8(conv3_3) # Shortcut\n",
    "        conv8_3 = self.depthnet.model8(conv8_up)\n",
    "        conv9_up = self.depthnet.model9up(conv8_3) + self.depthnet.model2short9(conv2_2) # Shortcut\n",
    "        conv9_3 = self.depthnet.model9(conv9_up)\n",
    "        conv10_up = self.depthnet.model10up(conv9_3) + self.depthnet.model1short10(conv1_2) # Shortcut\n",
    "        conv10_2 = self.depthnet.model10(conv10_up)\n",
    "        out_reg = self.depthnet.model_out(conv10_2)\n",
    "        return out_reg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training dataset from data/sunrgbd_nyu/train.txt with size 1159.\n",
      "Loaded dev dataset from data/sunrgbd_nyu/dev.txt with size 145.\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from depthnet.dataset import DepthDataset, ToFloat, CenterCrop, RandomCrop, AddDepthHist, ToTensor\n",
    "    \n",
    "    \n",
    "# TODO: Do this the right way:\n",
    "\n",
    "def get_global_stats(self, outFile=None, writeFile=False):\n",
    "    \"\"\"Calculate mean and variance of each rgb channel.\n",
    "\n",
    "    Optionally caches the result of this calculation in outfile so it doesn't need to be done each\n",
    "    time the dataset is loaded.\n",
    "    \"\"\"\n",
    "    S = np.zeros(3)\n",
    "    S_sq = np.zeros(3)\n",
    "    npixels = 0.\n",
    "    for depthFile, rgbFile in self.data:\n",
    "        rgbImg = Image.open(os.path.join(self.dataDir, rgbFile))\n",
    "        rgbImg = np.asarray(rgbImg, dtype=np.uint16)\n",
    "#             print(rgbImg[0:10, 0:10, :])\n",
    "\n",
    "        npixels += rgbImg.shape[0]*rgbImg.shape[1]\n",
    "        for channel in range(rgbImg.shape[2]):\n",
    "            S[channel] += np.sum(rgbImg[:,:,channel])\n",
    "            S_sq[channel] += np.sum((rgbImg[:,:,channel])**2)\n",
    "    mean = S/npixels\n",
    "    var = S_sq/npixels - mean**2\n",
    "\n",
    "    # Load full dataset (memory-intensive)\n",
    "#         full = []\n",
    "#         for depthFile, rgbFile in self.data:\n",
    "#             rgbImg = Image.open(os.path.join(self.dataDir, rgbFile))\n",
    "#             rgbImg = np.asarray(rgbImg, dtype=np.uint16)\n",
    "#             full.append(rgbImg)\n",
    "\n",
    "#         a = np.array(full)\n",
    "#         mean_true = np.mean(a, axis=(0, 1, 2))\n",
    "#         var_true = np.var(a, axis=(0, 1, 2))\n",
    "#         print(\"actual mean and variance: {} {}\".format(mean_true, var_true))\n",
    "#         print(a.shape)\n",
    "    return mean, var\n",
    "\n",
    "DepthDataset.get_global_stats = get_global_stats\n",
    "class NormalizeRGB(object):\n",
    "    def __init__(self, mean, var):\n",
    "        \"\"\"\n",
    "        mean - np.array of size 3 - the means of the three color channels over the whole (training) dataset\n",
    "        var - np.array of size 3 - the variances of the three color channels over the whole (training) dataset\n",
    "        \"\"\"\n",
    "        self.mean = mean\n",
    "        self.var = var\n",
    "    def __call__(self, sample):\n",
    "        sample[\"rgb\"] -= self.mean\n",
    "        sample[\"rgb\"] /= np.sqrt(self.var)\n",
    "#         print(sample[\"rgb\"][0:10, 0:10, 0])\n",
    "        return sample\n",
    "# Load training data\n",
    "train_txt = \"data/sunrgbd_nyu/train.txt\"\n",
    "trainDir = \"data/sunrgbd_nyu\"\n",
    "train = DepthDataset(train_txt, trainDir)\n",
    "mean, var = train.get_global_stats()\n",
    "train.transform = transforms.Compose([ToFloat(),\n",
    "                                      RandomCrop((320, 400)),\n",
    "                                      AddDepthHist(bins=800//3, range=(0,8)),\n",
    "                                      NormalizeRGB(mean, var),\n",
    "                                      ToTensor(),\n",
    "                                     ])\n",
    "#                      transform=transforms.Compose([ToFloat(), Crop_8(), ToFloat(), ToTensor()])\n",
    "#                      transform=transforms.Compose([ToFloat(), Crop_small(), ToFloat(), ToTensor()])\n",
    "                    \n",
    "# for i in range(len(train)):\n",
    "#     if np.any((train[i][\"depth\"] < 0).numpy()):\n",
    "#         print(\"yo wat\")\n",
    "#         break\n",
    "print(\"Loaded training dataset from {} with size {}.\".format(train_txt, len(train)))\n",
    "\n",
    "dev_txt = \"data/sunrgbd_nyu/dev.txt\"\n",
    "devDir = \"data/sunrgbd_nyu\"\n",
    "dev = DepthDataset(dev_txt, devDir, \n",
    "                     transform = transforms.Compose([ToFloat(),\n",
    "                                                     CenterCrop((400, 320)),\n",
    "                                                     AddDepthHist(bins=800//3, range=(0,8)),\n",
    "                                                     NormalizeRGB(mean, var),\n",
    "                                                     ToTensor(),\n",
    "                                                    ])\n",
    "#                      transform=transforms.Compose([ToFloat(), Crop_8(), ToTensor()])\n",
    "#                      transform=transforms.Compose([ToFloat(), Crop_small(), ToTensor()])\n",
    "                    )\n",
    "print(\"Loaded dev dataset from {} with size {}.\".format(dev_txt, len(dev)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded checkpoint 'checkpoints_hints/checkpoint_epoch_49.pth.tar' (trained for 49 epochs)\n",
      "loaded checkpointfile: checkpoints_hints/checkpoint_epoch_49.pth.tar\n",
      "start_epoch: 50\n",
      "global_it: 0\n",
      "optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 1e-05\n",
      "    lr: 1e-05\n",
      "    weight_decay: 0\n",
      ")\n",
      "batch_size: 10\n",
      "num_epochs: 50\n",
      "learning rate (initial): 1e-06\n",
      "scheduler: {'milestones': [], 'gamma': 0.1, 'base_lrs': [1e-05], 'last_epoch': -1}\n"
     ]
    }
   ],
   "source": [
    "# Set up training.\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader\n",
    "from depthnet.utils import save_checkpoint, validate\n",
    "\n",
    "def validate(loss, model, val_loader):\n",
    "    \"\"\"Computes the validation error of the model on the validation set.\n",
    "    val_loader should be a DataLoader.\n",
    "    \n",
    "    Returns an ordinary number (i.e. not a tensor)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    it = None\n",
    "    losses = []\n",
    "    for it, data in enumerate(val_loader):\n",
    "        depth = data[\"depth\"].float()\n",
    "        rgb = data[\"rgb\"].float()\n",
    "        if torch.cuda.is_available():\n",
    "            depth = depth.cuda()\n",
    "            rgb = rgb.cuda()\n",
    "        if \"hist\" in data:\n",
    "#             print(data)\n",
    "            hist = data[\"hist\"].float()\n",
    "            if torch.cuda.is_available():\n",
    "                hist = hist.cuda()\n",
    "#             print(hist)\n",
    "            output = model(rgb, hist)\n",
    "        else:\n",
    "            output = model(rgb)\n",
    "        losses.append(loss(output, depth).item())\n",
    "    nbatches = it+1\n",
    "    return sum(losses)/nbatches\n",
    "\n",
    "checkpointfile = \"checkpoints_hints/checkpoint_epoch_49.pth.tar\"\n",
    "# lam = 1e-8 # Weight decay parameter for L2 regularization\n",
    "learning_rate = 1e-6\n",
    "num_epochs = 50\n",
    "batch_size = 10\n",
    "val_batch_size=5\n",
    "\n",
    "# Build model and loss\n",
    "# Hyperparameters\n",
    "input_nc = 3\n",
    "output_nc = 1\n",
    "nbins=800//3\n",
    "\n",
    "dn = DepthNet(input_nc, output_nc)\n",
    "model = DepthNetWithHints(dn, hist_len=nbins, num_hints_layers=4)\n",
    "\n",
    "# Tensorboardx\n",
    "writer = SummaryWriter(comment=\"with_hints\")\n",
    "# data_trainloss = \"data/trainloss\"\n",
    "# data_valloss = \"data/valloss\"\n",
    "# Image = \"Image\"\n",
    "\n",
    "#################\n",
    "# Loss function #\n",
    "#################\n",
    "\n",
    "def berhu_loss(prediction, target):\n",
    "    diff = prediction - target\n",
    "    threshold = 0.2*torch.max(torch.abs(prediction - target))\n",
    "    c = threshold.detach()\n",
    "    l2_part = torch.sum((diff**2 + c**2))/(2*c)\n",
    "    l1_part = torch.sum(torch.abs(diff))\n",
    "    return l1_part+l2_part\n",
    "\n",
    "# loss = nn.SmoothL1Loss()\n",
    "# loss = berhu_loss\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    loss.cuda()\n",
    "\n",
    "# Checkpointing\n",
    "if checkpointfile is not None:\n",
    "    if torch.cuda.is_available():\n",
    "        checkpoint = torch.load(checkpointfile)\n",
    "    else:\n",
    "        # Load GPU model on CPU\n",
    "        checkpoint = torch.load(checkpointfile,\n",
    "                                map_location=lambda storage,\n",
    "                                loc: storage)\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_loss = checkpoint['best_loss']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer.load_state_dict(checkpoint['optim_state_dict'])\n",
    "    trainlosses = checkpoint['trainlosses']\n",
    "    vallosses = checkpoint['vallosses']\n",
    "    for i, trainloss in enumerate(trainlosses): # For tensorboardx\n",
    "        writer.add_scalar(\"data/trainloss\", trainloss, i)\n",
    "    vallosses = checkpoint['vallosses']\n",
    "    for i, valloss in enumerate(vallosses): # For tensorboardx\n",
    "        writer.add_scalar(\"data/valloss\", valloss, i)\n",
    "    global_it = len(trainlosses)\n",
    "    print(\"=> loaded checkpoint '{}' (trained for {} epochs)\".format(checkpointfile, checkpoint['epoch']))\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    global_it = 0 # Track global iterations\n",
    "    best_loss = torch.FloatTensor([float('inf')])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    trainlosses = []\n",
    "    vallosses = []\n",
    "    # Initialize weights:\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"conv\" in name and \"weight\" in name:\n",
    "#             print(name)\n",
    "            nn.init.xavier_normal_(param)\n",
    "        if \"norm\" in name and \"weight\" in name:\n",
    "#             print(name)\n",
    "            nn.init.constant_(param, 1)\n",
    "        elif \"bias\" in name:\n",
    "            nn.init.constant_(param, 0)\n",
    "            \n",
    "# Scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[], gamma=0.1)\n",
    "\n",
    "# Print summary of setup:\n",
    "print(\"loaded checkpointfile: {}\".format(checkpointfile))\n",
    "print(\"start_epoch: {}\".format(start_epoch))\n",
    "print(\"global_it: {}\".format(global_it))\n",
    "print(\"optimizer: {}\".format(optimizer))\n",
    "print(\"batch_size: {}\".format(batch_size))\n",
    "print(\"num_epochs: {}\".format(num_epochs))\n",
    "print(\"learning rate (initial): {}\".format(learning_rate))\n",
    "print(\"scheduler: {}\".format(scheduler.state_dict()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50\n",
      "\titeration: 0\ttrain loss: 0.42086899280548096\n",
      "\titeration: 10\ttrain loss: 0.6157236695289612\n",
      "\titeration: 20\ttrain loss: 0.7417676448822021\n",
      "\titeration: 30\ttrain loss: 0.39305663108825684\n",
      "\titeration: 40\ttrain loss: 0.48142266273498535\n",
      "\titeration: 50\ttrain loss: 0.5271826982498169\n",
      "\titeration: 60\ttrain loss: 0.45935332775115967\n",
      "\titeration: 70\ttrain loss: 0.46585604548454285\n",
      "\titeration: 80\ttrain loss: 0.5897435545921326\n",
      "\titeration: 90\ttrain loss: 0.6782640814781189\n",
      "\titeration: 100\ttrain loss: 0.3469370901584625\n",
      "\titeration: 110\ttrain loss: 0.5648832321166992\n",
      "End epoch 50\tval loss: 0.7640022193563396\n",
      "=> Saving checkpoint to: checkpoints_hints/checkpoint_epoch_50.pth.tar\n",
      "epoch: 51\n",
      "\titeration: 0\ttrain loss: 0.4101755917072296\n",
      "\titeration: 10\ttrain loss: 0.5434679985046387\n",
      "\titeration: 20\ttrain loss: 0.5831940174102783\n",
      "\titeration: 30\ttrain loss: 0.5146448612213135\n",
      "\titeration: 40\ttrain loss: 0.4713344871997833\n",
      "\titeration: 50\ttrain loss: 0.6203109622001648\n",
      "\titeration: 60\ttrain loss: 0.7447916269302368\n",
      "\titeration: 70\ttrain loss: 0.5050836205482483\n",
      "\titeration: 80\ttrain loss: 0.5566117763519287\n",
      "\titeration: 90\ttrain loss: 0.6033474802970886\n",
      "\titeration: 100\ttrain loss: 0.483550101518631\n",
      "\titeration: 110\ttrain loss: 0.4093293845653534\n",
      "End epoch 51\tval loss: 0.7868815177473528\n",
      "=> Saving checkpoint to: checkpoints_hints/checkpoint_epoch_51.pth.tar\n",
      "epoch: 52\n",
      "\titeration: 0\ttrain loss: 0.5547097325325012\n",
      "\titeration: 10\ttrain loss: 0.7486071586608887\n",
      "\titeration: 20\ttrain loss: 0.3803035020828247\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# Run the training #\n",
    "####################\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(dev, batch_size=val_batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "# Normalize rgb input\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "    print(\"epoch: {}\".format(epoch))\n",
    "    data = None\n",
    "    output = None\n",
    "    for it, data in enumerate(train_loader):\n",
    "        depth = data[\"depth\"].float()\n",
    "        rgb = data[\"rgb\"].float()\n",
    "        hist = data[\"hist\"].float()\n",
    "\n",
    "#         print(rgb.shape)\n",
    "#         print(depth.shape)\n",
    "#         print(hist.shape)\n",
    "        if torch.cuda.is_available():\n",
    "            depth = depth.cuda()\n",
    "            rgb = rgb.cuda()\n",
    "            hist = hist.cuda()\n",
    "        # New batch\n",
    "#         print(rgb.dtype)\n",
    "        scheduler.optimizer.zero_grad()\n",
    "\n",
    "        # Normalize rgb input\n",
    "        output = model(rgb, hist)\n",
    "        \n",
    "        # Save the first batch output of every epoch\n",
    "        \n",
    "#         a = list(model.parameters())[0].clone()\n",
    "\n",
    "        trainloss = loss(output, depth)\n",
    "        trainloss.backward()\n",
    "#         print(list(model.parameters())[0].grad)\n",
    "        scheduler.optimizer.step()\n",
    "#         print(depth)\n",
    "#         print(output)\n",
    "#         b = list(model.parameters())[0].clone()\n",
    "\n",
    "        if not (it % 10):\n",
    "            print(\"\\titeration: {}\\ttrain loss: {}\".format(it, trainloss.item()))\n",
    "        trainlosses.append(trainloss.item())\n",
    "#         print(trainloss.item())\n",
    "#         print(writer)\n",
    "        writer.add_scalar(\"data/trainloss\", trainloss.item(), global_it)\n",
    "        \n",
    "        # TESTING:\n",
    "#         if not ((it + 1) % 5):\n",
    "#             # Stop after 5 batches\n",
    "#             break\n",
    "\n",
    "#         print(torch.equal(a.data, b.data))\n",
    "        global_it += 1\n",
    "    # Checkpointing\n",
    "    # Get bool not ByteTensor\"\n",
    "    valloss = validate(loss, model, val_loader)\n",
    "    print(\"End epoch {}\\tval loss: {}\".format(epoch, valloss))\n",
    "    vallosses.append(valloss)\n",
    "    writer.add_scalar(\"data/valloss\", valloss, epoch)\n",
    "\n",
    "    # Save the last batch output of every epoch\n",
    "    rgb_input = vutils.make_grid(data[\"rgb\"], nrow=batch_size, normalize=True, scale_each=True)\n",
    "    writer.add_image('image/rgb_input', rgb_input, epoch)\n",
    "    \n",
    "    depth_truth = vutils.make_grid(data[\"depth\"], nrow=batch_size, normalize=True, scale_each=True)\n",
    "    writer.add_image('image/depth_truth', depth_truth, epoch)\n",
    "    \n",
    "    depth_output = vutils.make_grid(output, nrow=batch_size, normalize=True, scale_each=True)\n",
    "    writer.add_image('image/depth_output', depth_output, epoch)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        writer.add_histogram(name, param.clone().cpu().data.numpy(), global_it)\n",
    "#     save_images(data[\"rgb\"], data[\"depth\"], output, outputDir=\"images\", filename=\"epoch_{}\".format(epoch))\n",
    "    \n",
    "    is_best = bool(trainloss.data.cpu().numpy() < best_loss.numpy())\n",
    "    # Get greater Tensor to keep track best acc\n",
    "    best_loss = torch.FloatTensor(min(trainloss.data.cpu().numpy(), best_loss.numpy()))\n",
    "    # Save checkpoint\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch,\n",
    "        'global_it' : global_it,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_loss': best_loss,\n",
    "        'optim_state_dict': optimizer.state_dict(),\n",
    "        'trainlosses': trainlosses,\n",
    "        'vallosses': vallosses\n",
    "    }, is_best, filename=\"checkpoints_hints/checkpoint_epoch_{}.pth.tar\".format(epoch), always_save=True)\n",
    "\n",
    "# Close tensorboardX    \n",
    "# writer.export_scalars_to_json(\"./all_scalars.json\") # for other processing\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
