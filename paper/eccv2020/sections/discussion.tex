In summary, we demonstrate a method to greatly improve depth estimates from
monocular depth estimators by correcting the scale ambiguity errors inherent
with such techniques.  Our approach produces depth maps with accurate absolute
depth, and helps the generalization of neural networks for MDE across scene
types, including on data captured with our hardware prototype.  Moreover, we
require only minimal additional sensing hardware; we show that a single measurement
histogram from a diffused SPAD sensor contains enough information about global
scene geometry to correct errors in monocular depth estimates.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Limitations}

The performance of our method is highly dependent on the accuracy of the initial
depth map of the MDE algorithm.
%Since our method relies on the monocular depth estimate, the accuracy of the output depth map also relies on the performance of the initial depth estimate.
Our results demonstrate that when the MDE technique produces a depth map
with good ordinal accuracy, where the ordering of object depths is roughly correct, the
depth estimate can be corrected to produce accurate absolute depth. However, if
the ordering of the initial depths is not correct, these errors will not be
corrected by our histogram matching procedure and may propagate to the final
output depth map.

\edit{In the optically diffused configuration, the laser
power is spread out over the entire scene. Accordingly, for distant scene points very
little light may return to the SPAD and it may be difficult to accurately
capture distant scene geometry in the histogram. Thus our method is best suited
to short to medium-range scenes with a reasonable power budget for the diffused
illumination.} \edit{For example, assuming an indoor scene with
fluorescent bulbs and an ambient spectral irradiance of $I_A =
2~\text{mW}/\text{m}^2$ (across the 1 nm pass band of a spectral filter matched
to the laser), we find that the laser power required to achieve a minimum SBR of 5 for a
diffuse scene at $r = 2$ m
and a field of view of $\theta = 40^\circ$ can be calculated as}
%
\begin{equation}
  P_{\text{min}} = I_A \cdot 4 r^2 \tan^2(\theta/2) \cdot SBR_{\text{min}},
\end{equation} 
\edit{giving $P_{\text{min}}=21$ mW. We note that this is significantly 
less than the 60 mW used by the Kinect sensor to diffusely illuminate a scene.}

\edit{Under these scene parameters, we can compute the total incident 
  flux on the detector per second (derived in~\cite{mcmanamon2012ladar}) as}
  \begin{equation}
    P_R = P_T \cdot \rho \cdot \frac{A_{rec}}{\pi R^2} \cdot \eta
  \end{equation}
  \edit{where $P_T$ is the illumination power in the visible region, $\rho$ is its
  albedo, $A_{rec}$ is the area of the detector region, $R$ is the distance to
  the object, and $\eta$ is the quantum efficiency of the detector. For a range of 
  values similar to those of our experiments (detailed in the supplement), we find that
  the ratio of photon detections to illumination pulses is roughly 4\%, or well within 
  low-flux regime where SPAD pileup is negligible~\cite{oconnortcspc}.
  We also empirically validate that our method works with a diffused setup and an output power of 
  approximately 25 mW without significant pileup effects as shown in Figure~\ref{fig:captured_diffuse_small} and detailed in the supplement.
  } \edit{However, even for operation in the high-flux regime, pileup can be mitigated  
    by reducing the amount of incident light, for example by reducing the aperture size or 
    using neutral density filters, or by using a pileup correction~\cite{Heide:2018,Rapp:2019}.}

  %\edit{Note
  % that this calculation involves only on the signal-to-background photon ratio,
  % which is independent of the actual number of counts recorded per exposure. In
  % other words, pileup and laser energy concerns may 
  % be addressed entirely independently. As for acquisition time, at a 10 MHz
  % pulse rate with
  % a 5\% photon detection rate, it takes just two seconds to acquire a million
  % photon events.} 

% \edit{Furthermore, our system uses a SPAD and is therefore susceptible to pileup.
% Because the SPAD must be quenched before it can
% detect another photon, earlier photons
% from closer objects may systematically prevent detection of photons from further
% objects, causing strong distortion of the transient. Fortunately, at the expense of
% longer acquisition times, pileup can be mitigated
% to a large extent by operating in the low-flux regime
% where the number of photon events per second is controlled to be at most 5\% the
% number of laser pulses per second~\cite{oconnortcspc}. Finally, even in
% the high-flux regime, it is sometimes possible to correct pileup computationally~\cite{Heide:2018}.}

% \edit{While our prototype shows results for scanned/summed measurements, our scanning prototype operates in the low-flux regime,
% avoiding pileup. A hypothetical diffused SPAD prototype could also operate in
% this low-flux regime, which would yield identical measurement models and identical
% results. Figure~\ref{fig:hardware}(b)-(c) empirically shows that collecting and summing scanned
% measurements is nearly identical to capturing diffused measurements. ?!We show
% empirical comparisons between the two modes on a more complex scene in the supplement?!}

% \edit{Even if diffused and scanned/summed measurements are equivalent
%   computationally, they are not equivalent energetically.  This number is dependent on scene albedo, but all time-of-flight imaging
% systems share this dependency.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Future Work}
While our hardware prototype is large, future work could miniaturize this
system. Our algorithm or similar sensor fusion algorithms could
also be integrated into electronics that already contain the required hardware
components, for example, existing cell phones with single-pixel SPAD
proximity sensors and RGB cameras.

Other methods for extracting scene information from a depth histogram could be
employed, including learning-based methods to combine the MDE estimates and
histogram. One might even consider sensing regimes where the number of
returning signal photons is low, such as when the time-resolved detector and camera operate
at high framerates. While most MDE techniques are tailored to clean RGB images,  
the depth histogram could be used to help MDE
techniques generalize to noisy scenes under low-light conditions. 
 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Conclusions}
Since their introduction, monocular depth estimation algorithms have improved tremendously.  However, recent advances, which have generally relied on new network architectures or revised training procedures, have produced only modest performance improvements. 
In this work we dramatically improve the performance of several monocular depth estimation algorithms by fusing their estimates with depth histogram measurements. 
Such histograms are easy to capture using time-resolved single-photon detectors and are poised to become an important component of future low-cost imaging systems.
%Moving forward, sensor fusion is poised to become an even more important component of a variety of vision systems.
%Computational photography approaches may provide the measurements needed to achieve much greater performance improvements, especially as the sensors in devices we use every day become more diverse, complex, and increasingly rely on techniques based on sensor fusion and sophisticated computational algorithms.
 
%\begin{itemize}
%	\item new depth estimation CNNs achieve marginal gains, let's think about computational photography approaches that augment the measurements to achieve better performance
%	\item ...
%\end{itemize}