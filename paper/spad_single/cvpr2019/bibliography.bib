@Article{Alhashim2018a,
author = {Alhashim, Ibraheem and Wonka, Peter}, 
title = {High Quality Monocular Depth Estimation via Transfer Learning}, 
journal = {arXiv}, 
volume = {}, 
number = {}, 
pages = {1812.11941v2}, 
year = {2018}, 
abstract = {Accurate depth estimation from images is a fundamental task in many applications including scene understanding and reconstruction. Existing solutions for depth estimation often produce blurry approximations of low resolution. This paper presents a convolutional neural network for computing a high-resolution depth map given a single RGB image with the help of transfer learning. Following a standard encoder-decoder architecture, we leverage features extracted using high performing pre-trained networks when initializing our encoder along with augmentation and training strategies that lead to more accurate results. We show how, even for a very simple decoder, our method is able to achieve detailed high-resolution depth maps. Our network, with fewer parameters and training iterations, outperforms state-of-the-art on two datasets and also produces qualitatively better results that capture object boundaries more faithfully. Code and corresponding pre-trained weights are made publicly available.}, 
location = {}, 
keywords = {}}


@Proceedings{Eigen2014,
author = {Eigen, David and Puhrsch, Christian and Fergus, Rob}, 
editor = {}, 
title = {Depth map prediction from a single image using a multi-scale deep network}, 
booktitle = {Depth map prediction from a single image using a multi-scale deep network}, 
volume = {Advances in neural information processing systems}, 
publisher = {}, 
address = {}, 
pages = {2366-2374}, 
year = {2014}, 
abstract = {Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local …}, 
keywords = {}}

@Proceedings{Fu2018,
author = {Fu, Huan and Gong, Mingming and Wang, Chaohui and Batmanghelich, Kayhan and Tao, Dacheng}, 
editor = {}, 
title = {Deep ordinal regression network for monocular depth estimation}, 
booktitle = {Deep ordinal regression network for monocular depth estimation}, 
volume = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 
publisher = {}, 
address = {}, 
pages = {2002-2011}, 
year = {2018}, 
abstract = {Monocular depth estimation, which plays a crucial role in understanding 3D scene geometry, is an ill-posed prob-lem. Recent methods have gained significant improvement by exploring image-level information and hierarchical features from deep convolutional neural …}, 
keywords = {_tablet}}

@Proceedings{Godard2017,
author = {Godard, Clément and Mac Aodha, Oisin and Brostow, Gabriel J}, 
editor = {}, 
title = {Unsupervised monocular depth estimation with left-right consistency}, 
booktitle = {Unsupervised monocular depth estimation with left-right consistency}, 
volume = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 
publisher = {}, 
address = {}, 
pages = {270-279}, 
year = {2017}, 
abstract = {Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth …}, 
keywords = {}}

@Proceedings{Ha2016,
author = {Ha, Hyowon and Im, Sunghoon and Park, Jaesik and Jeon, Hae-Gon and Kweon, In So}, 
editor = {}, 
title = {High-Quality Depth from Uncalibrated Small Motion Clip}, 
booktitle = {High-Quality Depth from Uncalibrated Small Motion Clip}, 
volume = {}, 
publisher = {IEEE}, 
address = {}, 
pages = {}, 
year = {2016}, 
abstract = {}, 
keywords = {}}

@Article{Karsch2014,
author = {Karsch, K and Liu, C and Kang, SB}, 
title = {Depth Transfer: Depth Extraction from Video Using Non-Parametric Sampling.}, 
journal = {IEEE Trans Pattern Anal Mach Intell}, 
volume = {36}, 
number = {11}, 
pages = {2144–2158}, 
year = {2014}, 
abstract = {We describe a technique that automatically generates plausible depth maps from videos using non-parametric depth sampling. We demonstrate our technique in cases where past methods fail (non-translating cameras and dynamic scenes). Our technique is applicable to single images as well as videos. For videos, we use local motion cues to improve the inferred depth maps, while optical flow is used to ensure temporal depth consistency. For training and evaluation, we use a Kinect-based system to collect a large data set containing stereoscopic videos with known depths. We show that our depth estimation technique outperforms the state-of-the-art on benchmark databases. Our technique can be used to automatically convert a monoscopic video into stereo for 3D visualization, and we demonstrate this through a variety of visually pleasing results for indoor and outdoor scenes, including results from the feature film Charade.}, 
location = {}, 
keywords = {}}


@Proceedings{Laina2016,
author = {Laina, Iro and Rupprecht, Christian and Belagiannis, Vasileios and Tombari, Federico and Navab, Nassir}, 
editor = {}, 
title = {Deeper depth prediction with fully convolutional residual networks}, 
booktitle = {Deeper depth prediction with fully convolutional residual networks}, 
volume = {2016 Fourth international conference on 3D vision (3DV)}, 
publisher = {IEEE}, 
address = {}, 
pages = {239-248}, 
year = {2016}, 
abstract = {This paper addresses the problem of estimating the depth map of a scene given a single RGB image. We propose a fully convolutional architecture, encompassing residual learning, to model the ambiguous mapping between monocular images and depth maps. In order to …}, 
keywords = {CNN; Depth prediction}}

@Article{Lamb2010,
author = {Lamb, Robert and Buller, Gerald}, 
title = {Single-pixel imaging using 3D scanning time-of-flight photon counting}, 
journal = {SPIE Newsroom}, 
volume = {}, 
number = {}, 
pages = {}, 
year = {2010}, 
abstract = {}, 
location = {}, 
keywords = {}}


@Article{Lindell2018,
author = {Lindell, David B. and O’Toole, Matthew and Wetzstein, Gordon}, 
title = {Single-photon 3D Imaging with Deep Sensor Fusion}, 
journal = {ACM Trans. Graph.}, 
volume = {}, 
number = {}, 
pages = {1812.11941v2}, 
year = {2018}, 
abstract = {Accurate depth estimation from images is a fundamental task in many applications including scene understanding and reconstruction. Existing solutions for depth estimation often produce blurry approximations of low resolution. This paper presents a convolutional neural network for computing a high-resolution depth map given a single RGB image with the help of transfer learning. Following a standard encoder-decoder architecture, we leverage features extracted using high performing pre-trained networks when initializing our encoder along with augmentation and training strategies that lead to more accurate results. We show how, even for a very simple decoder, our method is able to achieve detailed high-resolution depth maps. Our network, with fewer parameters and training iterations, outperforms state-of-the-art on two datasets and also produces qualitatively better results that capture object boundaries more faithfully. Code and corresponding pre-trained weights are made publicly available.}, 
location = {}, 
keywords = {}}


@Article{Morovic2002,
author = {Morovic, Jan and Shaw, Julian and Sun, Pei-Li}, 
title = {A fast, non-iterative and exact histogram matching algorithm}, 
journal = {Pattern Recognition Letters}, 
volume = {23}, 
number = {1-3}, 
pages = {127–135}, 
year = {2002}, 
abstract = {}, 
location = {}, 
keywords = {}}


@Proceedings{Saxena2006,
author = {Saxena, Ashutosh and Chung, Sung H and Ng, Andrew Y}, 
editor = {Weiss, Y. and Schölkopf, B. and Platt, J. C.}, 
title = {Learning depth from single monocular images}, 
booktitle = {Learning depth from single monocular images}, 
volume = {Advances in neural information processing systems}, 
publisher = {MIT Press}, 
address = {}, 
pages = {1161-1168}, 
year = {2006}, 
abstract = {We consider the task of depth estimation from a single monocular image. We take a supervised learning approach to this problem, in which we begin by collecting a training set of monocular images (of unstructured outdoor environments which include forests, trees …}, 
keywords = {}}

@Article{Shin2015,
author = {Shin, D and Kirmani, A and Goyal, V K and Shapiro, J H}, 
title = {Photon-Efficient Computational 3-D and Reflectivity Imaging With Single-Photon Detectors}, 
journal = {IEEE Transactions on Computational Imaging}, 
volume = {1}, 
number = {2}, 
pages = {112–125}, 
year = {2015}, 
abstract = {}, 
location = {}, 
keywords = {computational imaging; convex optimization; image processing; 3-D imaging; 3D filtering; 3D imaging; avalanche photodiodes; block-matching; Detectors; first-photon imaging; image filtering; LIDAR; Lighting; low-light imaging; low-power active optical imaging; low-power electronics; median filtering; Noise; noise-tolerant active optical imaging; Optical imaging; photodetectors; photon-efficient computational 3D imaging; Photonics; Poisson noise; reflectivity; reflectivity imaging; signal-independent noise removal algorithms; single-photon detection; single-photon detectors; stochastic processes; Three-dimensional displays; time-of-flight imaging}}


@Article{Sun2016,
author = {Sun, MJ and Edgar, MP and Gibson, GM and Sun, B and Radwell, N and Lamb, R and Padgett, MJ}, 
title = {Single-pixel three-dimensional imaging with time-based depth resolution.}, 
journal = {Nat Commun}, 
volume = {7}, 
number = {}, 
pages = {12010}, 
year = {2016}, 
abstract = {Time-of-flight three-dimensional imaging is an important tool for applications such as object recognition and remote sensing. Conventional time-of-flight three-dimensional imaging systems frequently use a raster scanned laser to measure the range of each pixel in the scene sequentially. Here we show a modified time-of-flight three-dimensional imaging system, which can use compressed sensing techniques to reduce acquisition times, whilst distributing the optical illumination over the full field of view. Our system is based on a single-pixel camera using short-pulsed structured illumination and a high-speed photodiode, and is capable of reconstructing 128 × 128-pixel resolution three-dimensional scenes to an accuracy of ∼3 mm at a range of ∼5 m. Furthermore, by using a compressive sampling strategy, we demonstrate continuous real-time three-dimensional video with a frame-rate up to 12 Hz. The simplicity of the system hardware could enable low-cost three-dimensional imaging devices for precision ranging at wavelengths beyond the visible spectrum.}, 
location = {Department of Opto-electronic Engineering, Beihang University, Beijing 100191, China. SUPA, School of Physics and Astronomy, University of Glasgow, Glasgow G12 8QQ, UK. SUPA, School of Physics and Astronomy, University of Glasgow, Glasgow G12 8QQ, UK. SUPA, School of Physics and Astronomy, University of Glasgow, Glasgow G12 8QQ, UK. SUPA, School of Physics and Astronomy, University of Glasgow, Glasgow G12 8QQ, UK. SUPA, School of Physics and Astronomy, University of Glasgow, Glasgow G12 8QQ, UK. Selex ES, Edinburgh EH5 2XS UK. SUPA, School of Physics and Astronomy, University of Glasgow, Glasgow G12 8QQ, UK.}, 
keywords = {}}


@Article{Swoboda2013,
author = {Swoboda, Paul and Schnörr, Christoph}, 
title = {Convex Variational Image Restoration with Histogram Priors}, 
journal = {SIAM Journal of Imaging Sciences}, 
volume = {6}, 
number = {3}, 
pages = {1719–1735}, 
year = {2013}, 
abstract = {We present a novel variational approach to image restoration (e.g., denoising, inpainting, labeling) that enables to complement established variational approaches with a histogram-based prior enforcing closeness of the solution to some given empirical measure. By minimizing a single objective function, the approach utilizes simultaneously two quite different sources of information for restoration: spatial context in terms of some smoothness prior and non-spatial statistics in terms of the novel prior utilizing the Wasserstein distance between probability measures. We study the combination of the functional lifting technique with two different relaxations of the histogram prior and derive a jointly convex variational approach. Mathematical equivalence of both relaxations is established and cases where optimality holds are discussed. Additionally, we present an efficient algorithmic scheme for the numerical treatment of the presented model. Experiments using the basic total-variation based denoising approach as a case study demonstrate our novel regularization approach.}, 
location = {}, 
keywords = {ams subject classifications; 1; introduction; convex optimization; 10; 1137; 120897535; 65d18; 65k10; 68t45; 68u10; 90c06; 90c25; a broad range of; doi; low-level image; powerful variational approaches to; variational image processing; wasserstein distance}}


@Article{Xin2019,
author = {Xin, Shumian and Nousias, Sotiris and Kutulakos, Kiriakos N and Sankaranarayanan, Aswin C and Narasimhan, Srinivasa G and Gkioulekas, Ioannis}, 
title = {A theory of Fermat paths for non-line-of-sight shape reconstruction}, 
journal = {}, 
volume = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 
number = {}, 
pages = {6800–6809}, 
year = {2019}, 
abstract = {}, 
location = {}, 
keywords = {}}


@Proceedings{Xu2017,
author = {Xu, Dan and Ricci, Elisa and Ouyang, Wanli and Wang, Xiaogang and Sebe, Nicu}, 
editor = {}, 
title = {Multi-scale continuous crfs as sequential deep networks for monocular depth estimation}, 
booktitle = {Multi-scale continuous crfs as sequential deep networks for monocular depth estimation}, 
volume = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 
publisher = {}, 
address = {}, 
pages = {5354-5362}, 
year = {2017}, 
abstract = {This paper addresses the problem of depth estimation from a single still image. Inspired by recent works on multi-scale convolutional neural networks (CNN), we propose a deep model which fuses complementary information derived from multiple CNN side outputs. Different …}, 
keywords = {_tablet}}

@Proceedings{Zoran2015,
author = {Zoran, Daniel and Isola, Phillip and Krishnan, Dilip and Freeman, William T.}, 
editor = {}, 
title = {Learning Ordinal Relationships for Mid-Level Vision}, 
booktitle = {Learning Ordinal Relationships for Mid-Level Vision}, 
volume = {}, 
publisher = {IEEE}, 
address = {}, 
pages = {}, 
year = {2015}, 
abstract = {}, 
keywords = {}}

@Proceedings{,
author = {Xu, Dan and Wang, Wei and Tang, Hao and Liu, Hong and Sebe, Nicu and Ricci, Elisa}, 
editor = {}, 
title = {Structured attention guided convolutional neural fields for monocular depth estimation}, 
booktitle = {Structured attention guided convolutional neural fields for monocular depth estimation}, 
volume = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 
publisher = {}, 
address = {}, 
pages = {3917-3925}, 
year = {2018}, 
abstract = {Recent works have shown the benefit of integrating Conditional Random Fields (CRFs) models into deep architectures for improving pixel-level prediction tasks. Following this line of research, in this paper we introduce a novel approach for monocular depth estimation …}, 
keywords = {}}

@Proceedings{,
author = {Hao, Zhixiang and Li, Yu and You, Shaodi and Lu, Feng}, 
editor = {}, 
title = {Detail Preserving Depth Estimation from a Single Image Using Attention Guided Networks}, 
booktitle = {Detail Preserving Depth Estimation from a Single Image Using Attention Guided Networks}, 
volume = {2018 International Conference on 3D Vision (3DV)}, 
publisher = {IEEE}, 
address = {}, 
pages = {304-313}, 
year = {2018}, 
abstract = {Convolutional Neural Networks have demonstrated superior performance on single image depth estimation in recent years. These works usually use stacked spatial pooling or strided convolution to get high-level information which are common practices in classification task. However, depth estimation is a dense prediction problem and low-resolution feature maps usually generate blurred depth map which is undesirable in application. In order to produce high quality depth map, say clean and accurate, we propose a network consists of a Dense …}, 
keywords = {}}

