@Article{Alhashim2018,
author = {Alhashim, Ibraheem and Wonka, Peter}, 
title = {High Quality Monocular Depth Estimation via Transfer Learning}, 
journal = {arXiv}, 
volume = {}, 
number = {}, 
pages = {1812.11941v2}, 
year = {2018}, 
abstract = {Accurate depth estimation from images is a fundamental task in many applications including scene understanding and reconstruction. Existing solutions for depth estimation often produce blurry approximations of low resolution. This paper presents a convolutional neural network for computing a high-resolution depth map given a single RGB image with the help of transfer learning. Following a standard encoder-decoder architecture, we leverage features extracted using high performing pre-trained networks when initializing our encoder along with augmentation and training strategies that lead to more accurate results. We show how, even for a very simple decoder, our method is able to achieve detailed high-resolution depth maps. Our network, with fewer parameters and training iterations, outperforms state-of-the-art on two datasets and also produces qualitatively better results that capture object boundaries more faithfully. Code and corresponding pre-trained weights are made publicly available.}, 
location = {}, 
keywords = {}}


@Proceedings{Eigen2014,
author = {Eigen, David and Puhrsch, Christian and Fergus, Rob}, 
editor = {}, 
title = {Depth map prediction from a single image using a multi-scale deep network}, 
booktitle = {Depth map prediction from a single image using a multi-scale deep network}, 
volume = {Advances in neural information processing systems}, 
publisher = {}, 
address = {}, 
pages = {2366-2374}, 
year = {2014}, 
abstract = {Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local …}, 
keywords = {}}

@Proceedings{Fu2018,
author = {Fu, Huan and Gong, Mingming and Wang, Chaohui and Batmanghelich, Kayhan and Tao, Dacheng}, 
editor = {}, 
title = {Deep ordinal regression network for monocular depth estimation}, 
booktitle = {Deep ordinal regression network for monocular depth estimation}, 
volume = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 
publisher = {}, 
address = {}, 
pages = {2002-2011}, 
year = {2018}, 
abstract = {Monocular depth estimation, which plays a crucial role in understanding 3D scene geometry, is an ill-posed prob-lem. Recent methods have gained significant improvement by exploring image-level information and hierarchical features from deep convolutional neural …}, 
keywords = {_tablet}}

@Proceedings{Godard2017,
author = {Godard, Clément and Mac Aodha, Oisin and Brostow, Gabriel J}, 
editor = {}, 
title = {Unsupervised monocular depth estimation with left-right consistency}, 
booktitle = {Unsupervised monocular depth estimation with left-right consistency}, 
volume = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 
publisher = {}, 
address = {}, 
pages = {270-279}, 
year = {2017}, 
abstract = {Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth …}, 
keywords = {}}

@Book{Gonzalez2008,
author = {Gonzalez, Rafael C and Woods, Richard E}, 
title = {Digital Image Processing}, 
volume = {}, 
pages = {}, 
editor = {}, 
publisher = {}, 
address = {}, 
year = {2008}, 
abstract = {}, 
keywords = {}}

@Proceedings{Ha2016,
author = {Ha, Hyowon and Im, Sunghoon and Park, Jaesik and Jeon, Hae-Gon and Kweon, In So}, 
editor = {}, 
title = {High-Quality Depth from Uncalibrated Small Motion Clip}, 
booktitle = {High-Quality Depth from Uncalibrated Small Motion Clip}, 
volume = {}, 
publisher = {IEEE}, 
address = {}, 
pages = {}, 
year = {2016}, 
abstract = {}, 
keywords = {}}

@Proceedings{Hao2018,
author = {Hao, Zhixiang and Li, Yu and You, Shaodi and Lu, Feng}, 
editor = {}, 
title = {Detail Preserving Depth Estimation from a Single Image Using Attention Guided Networks}, 
booktitle = {Detail Preserving Depth Estimation from a Single Image Using Attention Guided Networks}, 
volume = {2018 International Conference on 3D Vision (3DV)}, 
publisher = {IEEE}, 
address = {}, 
pages = {304-313}, 
year = {2018}, 
abstract = {Convolutional Neural Networks have demonstrated superior performance on single image depth estimation in recent years. These works usually use stacked spatial pooling or strided convolution to get high-level information which are common practices in classification task. However, depth estimation is a dense prediction problem and low-resolution feature maps usually generate blurred depth map which is undesirable in application. In order to produce high quality depth map, say clean and accurate, we propose a network consists of a Dense …}, 
keywords = {}}

@Article{Horaud2016,
author = {Horaud, Radu and Hansard, Miles and Evangelidis, Georgios and Ménier, Clément}, 
title = {An overview of depth cameras and range scanners based on time-of-flight technologies}, 
journal = {Machine vision and applications}, 
volume = {27}, 
number = {7}, 
pages = {1005–1020}, 
year = {2016}, 
abstract = {}, 
location = {}, 
keywords = {}}


@Article{Karsch2014,
author = {Karsch, K and Liu, C and Kang, SB}, 
title = {Depth Transfer: Depth Extraction from Video Using Non-Parametric Sampling.}, 
journal = {IEEE Trans Pattern Anal Mach Intell}, 
volume = {36}, 
number = {11}, 
pages = {2144–2158}, 
year = {2014}, 
abstract = {We describe a technique that automatically generates plausible depth maps from videos using non-parametric depth sampling. We demonstrate our technique in cases where past methods fail (non-translating cameras and dynamic scenes). Our technique is applicable to single images as well as videos. For videos, we use local motion cues to improve the inferred depth maps, while optical flow is used to ensure temporal depth consistency. For training and evaluation, we use a Kinect-based system to collect a large data set containing stereoscopic videos with known depths. We show that our depth estimation technique outperforms the state-of-the-art on benchmark databases. Our technique can be used to automatically convert a monoscopic video into stereo for 3D visualization, and we demonstrate this through a variety of visually pleasing results for indoor and outdoor scenes, including results from the feature film Charade.}, 
location = {}, 
keywords = {}}


@Proceedings{Laina2016,
author = {Laina, Iro and Rupprecht, Christian and Belagiannis, Vasileios and Tombari, Federico and Navab, Nassir}, 
editor = {}, 
title = {Deeper depth prediction with fully convolutional residual networks}, 
booktitle = {Deeper depth prediction with fully convolutional residual networks}, 
volume = {2016 Fourth international conference on 3D vision (3DV)}, 
publisher = {IEEE}, 
address = {}, 
pages = {239-248}, 
year = {2016}, 
abstract = {This paper addresses the problem of estimating the depth map of a scene given a single RGB image. We propose a fully convolutional architecture, encompassing residual learning, to model the ambiguous mapping between monocular images and depth maps. In order to …}, 
keywords = {CNN; Depth prediction}}

@Article{Lamb2010,
author = {Lamb, Robert and Buller, Gerald}, 
title = {Single-pixel imaging using 3D scanning time-of-flight photon counting}, 
journal = {SPIE Newsroom}, 
volume = {}, 
number = {}, 
pages = {}, 
year = {2010}, 
abstract = {}, 
location = {}, 
keywords = {}}


@Article{Lindell2018,
author = {Lindell, David B. and O’Toole, Matthew and Wetzstein, Gordon}, 
title = {Single-photon 3D Imaging with Deep Sensor Fusion}, 
journal = {ACM Trans. Graph.}, 
volume = {}, 
number = {}, 
pages = {1812.11941v2}, 
year = {2018}, 
abstract = {Accurate depth estimation from images is a fundamental task in many applications including scene understanding and reconstruction. Existing solutions for depth estimation often produce blurry approximations of low resolution. This paper presents a convolutional neural network for computing a high-resolution depth map given a single RGB image with the help of transfer learning. Following a standard encoder-decoder architecture, we leverage features extracted using high performing pre-trained networks when initializing our encoder along with augmentation and training strategies that lead to more accurate results. We show how, even for a very simple decoder, our method is able to achieve detailed high-resolution depth maps. Our network, with fewer parameters and training iterations, outperforms state-of-the-art on two datasets and also produces qualitatively better results that capture object boundaries more faithfully. Code and corresponding pre-trained weights are made publicly available.}, 
location = {}, 
keywords = {}}


@Article{Morovic2002,
author = {Morovic, Jan and Shaw, Julian and Sun, Pei-Li}, 
title = {A fast, non-iterative and exact histogram matching algorithm}, 
journal = {Pattern Recognition Letters}, 
volume = {23}, 
number = {1-3}, 
pages = {127–135}, 
year = {2002}, 
abstract = {}, 
location = {}, 
keywords = {}}


@Article{Niclass2005,
author = {Niclass, C. and Rochas, A. and Besse, P.-A. and Charbon, E.}, 
title = {Design and characterization of a CMOS 3-D image sensor based on single photon avalanche diodes}, 
journal = {IEEE Journal of Solid-State Circuits}, 
volume = {40}, 
number = {9}, 
pages = {1847–1854}, 
year = {2005}, 
abstract = {}, 
location = {}, 
keywords = {}}


@Article{Nikolova2013,
author = {Nikolova, Mila and Wen, You-Wei and Chan, Raymond}, 
title = {Exact Histogram Specification for Digital Images Using a Variational Approach}, 
journal = {Journal of Mathematical Imaging and Vision
J Math Imaging Vis}, 
volume = {46}, 
number = {3}, 
pages = {309–325}, 
year = {2013}, 
abstract = {We consider the problem of exact histogram specification for digital (quantized) images. The goal is to transform the input digital image into an output (also digital) image that follows a prescribed histogram. Classical histogram modification methods are designed for real-valued images where all pixels have different values, so exact histogram specification is straightforward. Digital images typically have numerous pixels which share the same value. If one imposes the prescribed histogram to a digital image, usually there are numerous ways of assigning the prescribed values to the quantized values of the image. Therefore, exact histogram specification for digital images is an ill-posed problem. In order to guarantee that any prescribed histogram will be satisfied exactly, all pixels of the input digital image must be rearranged in a strictly ordered way. Further, the obtained strict ordering must faithfully account for the specific features of the input digital image. Such a task can be realized if we are able to extract additional representative information (called auxiliary attributes) from the input digital image. This is a real challenge in exact histogram specification for digital images. We propose a new method that efficiently provides a strict and faithful ordering for all pixel values. It is based on a well designed variational approach. Noticing that the input digital image contains quantization noise, we minimize a specialized objective function whose solution is a real-valued image with slightly reduced quantization noise, which remains very close to the input digital image. We show that all the pixels of this real-valued image can be ordered in a strict way with a very high probability. Then transforming the latter image into another digital image satisfying a specified histogram is an easy task. Numerical results show that our method outperforms by far the existing competing methods.}, 
location = {}, 
keywords = {Image processing; Convex minimization; Exact histogram specification; Minimizer analysis; Perturbation analysis; Restoration from quantization noise; Smooth nonlinear optimization; Strict-ordering; Variational methods}}


@Proceedings{OToole2017,
author = {O’Toole, M. and Heide, F. and Lindell, D. B. and Zang, K. and Diamond, S. and Wetzstein, G.}, 
editor = {}, 
title = {Reconstructing Transient Images from Single-Photon Sensors}, 
booktitle = {Reconstructing Transient Images from Single-Photon Sensors}, 
volume = {}, 
publisher = {}, 
address = {}, 
pages = {2289-2297}, 
year = {2017}, 
abstract = {Computer vision algorithms build on 2D images or 3D videos that capture dynamic events at the millisecond time scale. However, capturing and analyzing “transient images” at the picosecond scale-i.e., at one trillion frames per second-reveals unprecedented information about a scene and light transport within. This is not only crucial for time-of-flight range imaging, but it also helps further our understanding of light transport phenomena at a more fundamental level and potentially allows to revisit many assumptions made in different computer vision algorithms. In this work, we design and evaluate an imaging system that builds on single photon avalanche diode (SPAD) sensors to capture multi-path responses with picosecond-scale active illumination. We develop inverse methods that use modern approaches to deconvolve and denoise measurements in the presence of Poisson noise, and compute transient images at a higher quality than previously reported. The small form factor, fast acquisition rates, and relatively low cost of our system potentially makes transient imaging more practical for a range of applications.}, 
keywords = {computer vision; avalanche photodiodes; Photonics; _tablet; image reconstruction; photon counting; computer vision algorithms; dynamic event capture; high-speed optical techniques; Histograms; Holography; image capture; image resolution; Image resolution; image sensors; light transport phenomena; millisecond time scale; multipath response capture; picosecond-scale active illumination; scene; Sensors; single photon avalanche diode sensors; single-photon sensors; spatiotemporal phenomena; time-of-flight range imaging; Transient analysis; transient image reconstruction; transient imaging}}

@Proceedings{Rother2006,
author = {Rother, Carsten and Minka, Tom and Blake, Andrew and Kolmogorov, Vladimir}, 
editor = {}, 
title = {Cosegmentation of image pairs by histogram matching-incorporating a global constraint into mrfs}, 
booktitle = {Cosegmentation of image pairs by histogram matching-incorporating a global constraint into mrfs}, 
volume = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06) 1}, 
publisher = {IEEE}, 
address = {}, 
pages = {993-1000}, 
year = {2006}, 
abstract = {We introduce the term cosegmentation which denotes the task of segmenting simultaneously the common parts of an image pair. A generative model for cosegmentation is presented. Inference in the model leads to minimizing an energy with an MRF term …}, 
keywords = {}}

@Proceedings{Saxena2006,
author = {Saxena, Ashutosh and Chung, Sung H and Ng, Andrew Y}, 
editor = {Weiss, Y. and Schölkopf, B. and Platt, J. C.}, 
title = {Learning depth from single monocular images}, 
booktitle = {Learning depth from single monocular images}, 
volume = {Advances in neural information processing systems}, 
publisher = {MIT Press}, 
address = {}, 
pages = {1161-1168}, 
year = {2006}, 
abstract = {We consider the task of depth estimation from a single monocular image. We take a supervised learning approach to this problem, in which we begin by collecting a training set of monocular images (of unstructured outdoor environments which include forests, trees …}, 
keywords = {}}

@Article{Shin2015,
author = {Shin, D and Kirmani, A and Goyal, V K and Shapiro, J H}, 
title = {Photon-Efficient Computational 3-D and Reflectivity Imaging With Single-Photon Detectors}, 
journal = {IEEE Transactions on Computational Imaging}, 
volume = {1}, 
number = {2}, 
pages = {112–125}, 
year = {2015}, 
abstract = {}, 
location = {}, 
keywords = {computational imaging; convex optimization; image processing; 3-D imaging; 3D filtering; 3D imaging; avalanche photodiodes; block-matching; Detectors; first-photon imaging; image filtering; LIDAR; Lighting; low-light imaging; low-power active optical imaging; low-power electronics; median filtering; Noise; noise-tolerant active optical imaging; Optical imaging; photodetectors; photon-efficient computational 3D imaging; Photonics; Poisson noise; reflectivity; reflectivity imaging; signal-independent noise removal algorithms; single-photon detection; single-photon detectors; stochastic processes; Three-dimensional displays; time-of-flight imaging}}


@Article{Stoppa2007,
author = {Stoppa, David and Pancheri, Lucio and Scandiuzzo, Mauro and Gonzo, Lorenzo and Dalla Betta, Gian-Franco and Simoni, Andrea}, 
title = {A CMOS 3-D imager based on single photon avalanche diode}, 
journal = {IEEE Transactions on Circuits and Systems I: Regular Papers}, 
volume = {54}, 
number = {1}, 
pages = {4–12}, 
year = {2007}, 
abstract = {A 64-pixel linear array aimed at 3-D vision applications is implemented in a high-voltage 0.8 mum CMOS technology. The detection of the incident light signals is performed using photodiodes biased above breakdown voltage so that an extremely high sensitivity can be …}, 
location = {}, 
keywords = {}}


@Article{Sun2016,
author = {Sun, MJ and Edgar, MP and Gibson, GM and Sun, B and Radwell, N and Lamb, R and Padgett, MJ}, 
title = {Single-pixel three-dimensional imaging with time-based depth resolution.}, 
journal = {Nat Commun}, 
volume = {7}, 
number = {}, 
pages = {12010}, 
year = {2016}, 
abstract = {Time-of-flight three-dimensional imaging is an important tool for applications such as object recognition and remote sensing. Conventional time-of-flight three-dimensional imaging systems frequently use a raster scanned laser to measure the range of each pixel in the scene sequentially. Here we show a modified time-of-flight three-dimensional imaging system, which can use compressed sensing techniques to reduce acquisition times, whilst distributing the optical illumination over the full field of view. Our system is based on a single-pixel camera using short-pulsed structured illumination and a high-speed photodiode, and is capable of reconstructing 128 × 128-pixel resolution three-dimensional scenes to an accuracy of ∼3 mm at a range of ∼5 m. Furthermore, by using a compressive sampling strategy, we demonstrate continuous real-time three-dimensional video with a frame-rate up to 12 Hz. The simplicity of the system hardware could enable low-cost three-dimensional imaging devices for precision ranging at wavelengths beyond the visible spectrum.}, 
location = {Department of Opto-electronic Engineering, Beihang University, Beijing 100191, China. SUPA, School of Physics and Astronomy, University of Glasgow, Glasgow G12 8QQ, UK. SUPA, School of Physics and Astronomy, University of Glasgow, Glasgow G12 8QQ, UK. SUPA, School of Physics and Astronomy, University of Glasgow, Glasgow G12 8QQ, UK. SUPA, School of Physics and Astronomy, University of Glasgow, Glasgow G12 8QQ, UK. SUPA, School of Physics and Astronomy, University of Glasgow, Glasgow G12 8QQ, UK. Selex ES, Edinburgh EH5 2XS UK. SUPA, School of Physics and Astronomy, University of Glasgow, Glasgow G12 8QQ, UK.}, 
keywords = {}}


@Article{Swoboda2013,
author = {Swoboda, Paul and Schnörr, Christoph}, 
title = {Convex Variational Image Restoration with Histogram Priors}, 
journal = {SIAM Journal of Imaging Sciences}, 
volume = {6}, 
number = {3}, 
pages = {1719–1735}, 
year = {2013}, 
abstract = {We present a novel variational approach to image restoration (e.g., denoising, inpainting, labeling) that enables to complement established variational approaches with a histogram-based prior enforcing closeness of the solution to some given empirical measure. By minimizing a single objective function, the approach utilizes simultaneously two quite different sources of information for restoration: spatial context in terms of some smoothness prior and non-spatial statistics in terms of the novel prior utilizing the Wasserstein distance between probability measures. We study the combination of the functional lifting technique with two different relaxations of the histogram prior and derive a jointly convex variational approach. Mathematical equivalence of both relaxations is established and cases where optimality holds are discussed. Additionally, we present an efficient algorithmic scheme for the numerical treatment of the presented model. Experiments using the basic total-variation based denoising approach as a case study demonstrate our novel regularization approach.}, 
location = {}, 
keywords = {ams subject classifications; 1; introduction; convex optimization; 10; 1137; 120897535; 65d18; 65k10; 68t45; 68u10; 90c06; 90c25; a broad range of; doi; low-level image; powerful variational approaches to; variational image processing; wasserstein distance}}


@Article{Xin2019,
author = {Xin, Shumian and Nousias, Sotiris and Kutulakos, Kiriakos N and Sankaranarayanan, Aswin C and Narasimhan, Srinivasa G and Gkioulekas, Ioannis}, 
title = {A theory of Fermat paths for non-line-of-sight shape reconstruction}, 
journal = {}, 
volume = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 
number = {}, 
pages = {6800–6809}, 
year = {2019}, 
abstract = {}, 
location = {}, 
keywords = {}}


@Proceedings{Xu2017,
author = {Xu, Dan and Ricci, Elisa and Ouyang, Wanli and Wang, Xiaogang and Sebe, Nicu}, 
editor = {}, 
title = {Multi-scale continuous crfs as sequential deep networks for monocular depth estimation}, 
booktitle = {Multi-scale continuous crfs as sequential deep networks for monocular depth estimation}, 
volume = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 
publisher = {}, 
address = {}, 
pages = {5354-5362}, 
year = {2017}, 
abstract = {This paper addresses the problem of depth estimation from a single still image. Inspired by recent works on multi-scale convolutional neural networks (CNN), we propose a deep model which fuses complementary information derived from multiple CNN side outputs. Different …}, 
keywords = {_tablet}}

@Proceedings{Xu2018,
author = {Xu, Dan and Wang, Wei and Tang, Hao and Liu, Hong and Sebe, Nicu and Ricci, Elisa}, 
editor = {}, 
title = {Structured attention guided convolutional neural fields for monocular depth estimation}, 
booktitle = {Structured attention guided convolutional neural fields for monocular depth estimation}, 
volume = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 
publisher = {}, 
address = {}, 
pages = {3917-3925}, 
year = {2018}, 
abstract = {Recent works have shown the benefit of integrating Conditional Random Fields (CRFs) models into deep architectures for improving pixel-level prediction tasks. Following this line of research, in this paper we introduce a novel approach for monocular depth estimation …}, 
keywords = {}}

@Proceedings{Zoran2015,
author = {Zoran, Daniel and Isola, Phillip and Krishnan, Dilip and Freeman, William T.}, 
editor = {}, 
title = {Learning Ordinal Relationships for Mid-Level Vision}, 
booktitle = {Learning Ordinal Relationships for Mid-Level Vision}, 
volume = {}, 
publisher = {IEEE}, 
address = {}, 
pages = {}, 
year = {2015}, 
abstract = {}, 
keywords = {}}

@Article{Zhang2017,
author = {Zhang, Richard and Zhu, Jun-Yan and Isola, Phillip and Geng, Xinyang and Lin, Angela S and Yu, Tianhe and Efros, Alexei A}, 
title = {Real-Time User-Guided Image Colorization with Learned Deep Priors}, 
journal = {ACM Transactions on Graphics (TOG)}, 
volume = {9}, 
number = {4}, 
pages = {}, 
year = {2017}, 
abstract = {}, 
location = {}, 
keywords = {}}


@Article{Zhang2018,
author = {Zhang, C and Lindner, S and Antolovic, IM and Wolf, M and Charbon, E}, 
title = {A CMOS SPAD Imager with Collision Detection and 128 Dynamically Reallocating TDCs for Single-Photon Counting and 3D Time-of-Flight Imaging.}, 
journal = {Sensors (Basel)}, 
volume = {18}, 
number = {11}, 
pages = {}, 
year = {2018}, 
abstract = {Per-pixel time-to-digital converter (TDC) architectures have been exploited by single-photon avalanche diode (SPAD) sensors to achieve high photon throughput, but at the expense of fill factor, pixel pitch and readout efficiency. In contrast, TDC sharing architecture usually features high fill factor at small pixel pitch and energy efficient event-driven readout. While the photon throughput is not necessarily lower than that of per-pixel TDC architectures, since the throughput is not only decided by the TDC number but also the readout bandwidth. In this paper, a SPAD sensor with 32 × 32 pixels fabricated with a 180 nm CMOS image sensor technology is presented, where dynamically reallocating TDCs were implemented to achieve the same photon throughput as that of per-pixel TDCs. Each 4 TDCs are shared by 32 pixels via a collision detection bus, which enables a fill factor of 28% with a pixel pitch of 28.5 μm. The TDCs were characterized, obtaining the peak-to-peak differential and integral non-linearity of -0.07/+0.08 LSB and -0.38/+0.75 LSB, respectively. The sensor was demonstrated in a scanning light-detection-and-ranging (LiDAR) system equipped with an ultra-low power laser, achieving depth imaging up to 10 m at 6 frames/s with a resolution of 64 × 64 with 50 lux background light.}, 
location = {Quantum and Computer Engineering, Delft University of Technology, Mekelweg 4, 2628CD Delft, The Netherlands. c.zhang-10@tudelft.nl. Biomedical Optics Research Laboratory, University of Zurich, Rämistrasse 71, 8006 Zürich, Switzerland. scott.lindner@epfl.ch. Advanced Quantum Architecture Laboratory, École Polytechnique Fédérale de Lausanne (EPFL), Route Cantonale, 1015 Lausanne, Switzerland. scott.lindner@epfl.ch. Advanced Quantum Architecture Laboratory, École Polytechnique Fédérale de Lausanne (EPFL), Route Cantonale, 1015 Lausanne, Switzerland. michel.antolovic@epfl.ch. Biomedical Optics Research Laboratory, University of Zurich, Rämistrasse 71, 8006 Zürich, Switzerland. martin.wolf@usz.ch. Advanced Quantum Architecture Laboratory, École Polytechnique Fédérale de Lausanne (EPFL), Route Cantonale, 1015 Lausanne, Switzerland. edoardo.charbon@epfl.ch. Kavli Institute of Nanoscience, 2628CJ Delft, The Netherlands. edoardo.charbon@epfl.ch.}, 
keywords = {}}


@Article{Veerappan2011,
author = {Veerappan, Chockalingam and Richardson, Justin and Walker, Richard and Li, Day-Uey and Fishburn, Matthew W and Maruyama, Yuki and Stoppa, David and Borghetti, Fausto and Gersbach, Marek and Henderson, Robert K}, 
title = {A 160×128 single-photon image sensor with on-pixel 55ps 10b time-to-digital converter}, 
journal = {}, 
volume = {2011 IEEE International Solid-State Circuits Conference}, 
number = {}, 
pages = {312–314}, 
year = {2011}, 
abstract = {}, 
location = {}, 
keywords = {}}


@Article{Hoiem2005,
author = {Hoiem, Derek and Efros, Alexei A and Hebert, Martial}, 
title = {Automatic photo pop-up}, 
journal = {}, 
volume = {ACM transactions on graphics}, 
number = {TOG}, 
pages = {577–584}, 
year = {2005}, 
abstract = {This paper presents a fully automatic method for creating a 3D model from a single photograph. The model is made up of several texture-mapped planar billboards and has the complexity of a typical children’s pop-up book illustration. Our main insight is that instead of …}, 
location = {}, 
keywords = {}}


