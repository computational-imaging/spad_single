@Article{Alhashim2018,
author = {Alhashim, Ibraheem and Wonka, Peter}, 
title = {High Quality Monocular Depth Estimation via Transfer Learning}, 
journal = {arXiv:1812.11941v2}, 
year = {2018}, 
}


@inproceedings{Eigen2014,
author = {Eigen, David and Puhrsch, Christian and Fergus, Rob}, 
title = {Depth map prediction from a single image using a multi-scale deep network}, 
booktitle = {Advances in neural information processing systems}, 
pages = {2366-2374}, 
year = {2014}, 
}

@inproceedings{Fu2018,
author = {Fu, Huan and Gong, Mingming and Wang, Chaohui and Batmanghelich, Kayhan and Tao, Dacheng}, 
editor = {}, 
title = {Deep ordinal regression network for monocular depth estimation}, 
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 
pages = {2002-2011}, 
year = {2018}, 
}

@inproceedings{Godard2017,
author = {Godard, Clément and Mac Aodha, Oisin and Brostow, Gabriel J}, 
title = {Unsupervised monocular depth estimation with left-right consistency}, 
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 
pages = {270-279}, 
year = {2017}, 
}

@Book{Gonzalez2008,
author = {Gonzalez, Rafael C and Woods, Richard E}, 
title = {Digital Image Processing}, 
volume = {}, 
pages = {}, 
editor = {}, 
publisher = {}, 
address = {}, 
year = {2008}, 
abstract = {}, 
keywords = {}}

@inproceedings{Ha2016,
author = {Ha, Hyowon and Im, Sunghoon and Park, Jaesik and Jeon, Hae-Gon and Kweon, In So}, 
title = {High-Quality Depth from Uncalibrated Small Motion Clip}, 
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 
year = {2016}, 
}

@inproceedings{Hao2018,
author = {Hao, Zhixiang and Li, Yu and You, Shaodi and Lu, Feng}, 
editor = {}, 
title = {Detail Preserving Depth Estimation from a Single Image Using Attention Guided Networks}, 
booktitle = {International Conference on 3D Vision (3DV)}, 
pages = {304-313}, 
year = {2018}, 
}

@Article{Horaud2016,
author = {Horaud, Radu and Hansard, Miles and Evangelidis, Georgios and Ménier, Clément}, 
title = {An overview of depth cameras and range scanners based on time-of-flight technologies}, 
journal = {Machine vision and applications}, 
volume = {27}, 
number = {7}, 
pages = {1005–1020}, 
year = {2016}, 
abstract = {}, 
location = {}, 
keywords = {}}


@Article{Karsch2014,
author = {Karsch, K and Liu, C and Kang, SB}, 
title = {Depth Transfer: Depth Extraction from Video Using Non-Parametric Sampling.}, 
journal = {IEEE Trans Pattern Anal Mach Intell}, 
volume = {36}, 
number = {11}, 
pages = {2144–2158}, 
year = {2014}, 
abstract = {We describe a technique that automatically generates plausible depth maps from videos using non-parametric depth sampling. We demonstrate our technique in cases where past methods fail (non-translating cameras and dynamic scenes). Our technique is applicable to single images as well as videos. For videos, we use local motion cues to improve the inferred depth maps, while optical flow is used to ensure temporal depth consistency. For training and evaluation, we use a Kinect-based system to collect a large data set containing stereoscopic videos with known depths. We show that our depth estimation technique outperforms the state-of-the-art on benchmark databases. Our technique can be used to automatically convert a monoscopic video into stereo for 3D visualization, and we demonstrate this through a variety of visually pleasing results for indoor and outdoor scenes, including results from the feature film Charade.}, 
location = {}, 
keywords = {}}


@inproceedings{Laina2016,
author = {Laina, Iro and Rupprecht, Christian and Belagiannis, Vasileios and Tombari, Federico and Navab, Nassir}, 
editor = {}, 
title = {Deeper depth prediction with fully convolutional residual networks}, 
booktitle = {Fourth international conference on 3D vision (3DV)}, 
publisher = {IEEE}, 
address = {}, 
pages = {239-248}, 
year = {2016}, 
abstract = {This paper addresses the problem of estimating the depth map of a scene given a single RGB image. We propose a fully convolutional architecture, encompassing residual learning, to model the ambiguous mapping between monocular images and depth maps. In order to …}, 
keywords = {CNN; Depth prediction}}

@Article{Lamb2010,
author = {Lamb, Robert and Buller, Gerald}, 
title = {Single-pixel imaging using 3D scanning time-of-flight photon counting}, 
journal = {SPIE Newsroom}, 
volume = {}, 
number = {}, 
pages = {}, 
year = {2010}, 
abstract = {}, 
location = {}, 
keywords = {}}


@Article{Lindell2018,
author = {Lindell, David B. and O’Toole, Matthew and Wetzstein, Gordon}, 
title = {Single-photon 3D Imaging with Deep Sensor Fusion}, 
journal = {ACM Trans. Graph. (SIGGRAPH)}, 
volume = {}, 
number = {}, 
pages = {1812.11941v2}, 
year = {2018}, 
abstract = {Accurate depth estimation from images is a fundamental task in many applications including scene understanding and reconstruction. Existing solutions for depth estimation often produce blurry approximations of low resolution. This paper presents a convolutional neural network for computing a high-resolution depth map given a single RGB image with the help of transfer learning. Following a standard encoder-decoder architecture, we leverage features extracted using high performing pre-trained networks when initializing our encoder along with augmentation and training strategies that lead to more accurate results. We show how, even for a very simple decoder, our method is able to achieve detailed high-resolution depth maps. Our network, with fewer parameters and training iterations, outperforms state-of-the-art on two datasets and also produces qualitatively better results that capture object boundaries more faithfully. Code and corresponding pre-trained weights are made publicly available.}, 
location = {}, 
keywords = {}}


@Article{Morovic2002,
author = {Morovic, Jan and Shaw, Julian and Sun, Pei-Li}, 
title = {A fast, non-iterative and exact histogram matching algorithm}, 
journal = {Pattern Recognition Letters}, 
volume = {23}, 
number = {1-3}, 
pages = {127–135}, 
year = {2002}, 
abstract = {}, 
location = {}, 
keywords = {}}


@Article{Niclass2005,
author = {Niclass, C. and Rochas, A. and Besse, P.-A. and Charbon, E.}, 
title = {Design and characterization of a CMOS 3-D image sensor based on single photon avalanche diodes}, 
journal = {IEEE Journal of Solid-State Circuits}, 
volume = {40}, 
number = {9}, 
pages = {1847–1854}, 
year = {2005}, 
abstract = {}, 
location = {}, 
keywords = {}}


@Article{Nikolova2013,
author = {Nikolova, Mila and Wen, You-Wei and Chan, Raymond}, 
title = {Exact Histogram Specification for Digital Images Using a Variational Approach}, 
journal = {Journal of Mathematical Imaging and Vision
J Math Imaging Vis}, 
volume = {46}, 
number = {3}, 
pages = {309–325}, 
year = {2013}, 
abstract = {We consider the problem of exact histogram specification for digital (quantized) images. The goal is to transform the input digital image into an output (also digital) image that follows a prescribed histogram. Classical histogram modification methods are designed for real-valued images where all pixels have different values, so exact histogram specification is straightforward. Digital images typically have numerous pixels which share the same value. If one imposes the prescribed histogram to a digital image, usually there are numerous ways of assigning the prescribed values to the quantized values of the image. Therefore, exact histogram specification for digital images is an ill-posed problem. In order to guarantee that any prescribed histogram will be satisfied exactly, all pixels of the input digital image must be rearranged in a strictly ordered way. Further, the obtained strict ordering must faithfully account for the specific features of the input digital image. Such a task can be realized if we are able to extract additional representative information (called auxiliary attributes) from the input digital image. This is a real challenge in exact histogram specification for digital images. We propose a new method that efficiently provides a strict and faithful ordering for all pixel values. It is based on a well designed variational approach. Noticing that the input digital image contains quantization noise, we minimize a specialized objective function whose solution is a real-valued image with slightly reduced quantization noise, which remains very close to the input digital image. We show that all the pixels of this real-valued image can be ordered in a strict way with a very high probability. Then transforming the latter image into another digital image satisfying a specified histogram is an easy task. Numerical results show that our method outperforms by far the existing competing methods.}, 
location = {}, 
keywords = {Image processing; Convex minimization; Exact histogram specification; Minimizer analysis; Perturbation analysis; Restoration from quantization noise; Smooth nonlinear optimization; Strict-ordering; Variational methods}}


@inproceedings{OToole2017,
author = {O’Toole, M. and Heide, F. and Lindell, D. B. and Zang, K. and Diamond, S. and Wetzstein, G.}, 
title = {Reconstructing Transient Images from Single-Photon Sensors}, 
booktitle = {Conference on Computer Vision and Pattern Recognition}, 
volume = {}, 
publisher = {}, 
address = {}, 
pages = {2289-2297}, 
year = {2017}, 
}

@inproceedings{Rother2006,
author = {Rother, Carsten and Minka, Tom and Blake, Andrew and Kolmogorov, Vladimir}, 
editor = {}, 
title = {Cosegmentation of image pairs by histogram matching-incorporating a global constraint into mrfs}, 
booktitle = {Conference on Computer Vision and Pattern Recognition}, 
publisher = {IEEE}, 
address = {}, 
pages = {993-1000}, 
year = {2006}, 
abstract = {We introduce the term cosegmentation which denotes the task of segmenting simultaneously the common parts of an image pair. A generative model for cosegmentation is presented. Inference in the model leads to minimizing an energy with an MRF term …}, 
keywords = {}}

@inproceedings{Saxena2006,
author = {Saxena, Ashutosh and Chung, Sung H and Ng, Andrew Y}, 
title = {Learning depth from single monocular images}, 
booktitle = {Advances in neural information processing systems}, 
pages = {1161-1168}, 
year = {2006}, 
}

@Article{Shin2015,
author = {Shin, D and Kirmani, A and Goyal, V K and Shapiro, J H}, 
title = {Photon-Efficient Computational 3-D and Reflectivity Imaging With Single-Photon Detectors}, 
journal = {IEEE Transactions on Computational Imaging}, 
volume = {1}, 
number = {2}, 
pages = {112–125}, 
year = {2015}, 
abstract = {}, 
location = {}, 
keywords = {computational imaging; convex optimization; image processing; 3-D imaging; 3D filtering; 3D imaging; avalanche photodiodes; block-matching; Detectors; first-photon imaging; image filtering; LIDAR; Lighting; low-light imaging; low-power active optical imaging; low-power electronics; median filtering; Noise; noise-tolerant active optical imaging; Optical imaging; photodetectors; photon-efficient computational 3D imaging; Photonics; Poisson noise; reflectivity; reflectivity imaging; signal-independent noise removal algorithms; single-photon detection; single-photon detectors; stochastic processes; Three-dimensional displays; time-of-flight imaging}}


@Article{Stoppa2007,
author = {Stoppa, David and Pancheri, Lucio and Scandiuzzo, Mauro and Gonzo, Lorenzo and Dalla Betta, Gian-Franco and Simoni, Andrea}, 
title = {A CMOS 3-D imager based on single photon avalanche diode}, 
journal = {IEEE Transactions on Circuits and Systems I: Regular Papers}, 
volume = {54}, 
number = {1}, 
pages = {4–12}, 
year = {2007}, 
abstract = {A 64-pixel linear array aimed at 3-D vision applications is implemented in a high-voltage 0.8 mum CMOS technology. The detection of the incident light signals is performed using photodiodes biased above breakdown voltage so that an extremely high sensitivity can be …}, 
location = {}, 
keywords = {}}


@Article{Sun2016,
author = {Sun, MJ and Edgar, MP and Gibson, GM and Sun, B and Radwell, N and Lamb, R and Padgett, MJ}, 
title = {Single-pixel three-dimensional imaging with time-based depth resolution.}, 
journal = {Nat Commun}, 
volume = {7}, 
number = {}, 
pages = {12010}, 
year = {2016}, 
abstract = {Time-of-flight three-dimensional imaging is an important tool for applications such as object recognition and remote sensing. Conventional time-of-flight three-dimensional imaging systems frequently use a raster scanned laser to measure the range of each pixel in the scene sequentially. Here we show a modified time-of-flight three-dimensional imaging system, which can use compressed sensing techniques to reduce acquisition times, whilst distributing the optical illumination over the full field of view. Our system is based on a single-pixel camera using short-pulsed structured illumination and a high-speed photodiode, and is capable of reconstructing 128 × 128-pixel resolution three-dimensional scenes to an accuracy of ∼3 mm at a range of ∼5 m. Furthermore, by using a compressive sampling strategy, we demonstrate continuous real-time three-dimensional video with a frame-rate up to 12 Hz. The simplicity of the system hardware could enable low-cost three-dimensional imaging devices for precision ranging at wavelengths beyond the visible spectrum.}, 
location = {Department of Opto-electronic Engineering, Beihang University, Beijing 100191, China. SUPA, School of Physics and Astronomy, University of Glasgow, Glasgow G12 8QQ, UK. SUPA, School of Physics and Astronomy, University of Glasgow, Glasgow G12 8QQ, UK. SUPA, School of Physics and Astronomy, University of Glasgow, Glasgow G12 8QQ, UK. SUPA, School of Physics and Astronomy, University of Glasgow, Glasgow G12 8QQ, UK. SUPA, School of Physics and Astronomy, University of Glasgow, Glasgow G12 8QQ, UK. Selex ES, Edinburgh EH5 2XS UK. SUPA, School of Physics and Astronomy, University of Glasgow, Glasgow G12 8QQ, UK.}, 
keywords = {}}


@Article{Swoboda2013,
author = {Swoboda, Paul and Schnörr, Christoph}, 
title = {Convex Variational Image Restoration with Histogram Priors}, 
journal = {SIAM Journal of Imaging Sciences}, 
volume = {6}, 
number = {3}, 
pages = {1719–1735}, 
year = {2013}, 
abstract = {We present a novel variational approach to image restoration (e.g., denoising, inpainting, labeling) that enables to complement established variational approaches with a histogram-based prior enforcing closeness of the solution to some given empirical measure. By minimizing a single objective function, the approach utilizes simultaneously two quite different sources of information for restoration: spatial context in terms of some smoothness prior and non-spatial statistics in terms of the novel prior utilizing the Wasserstein distance between probability measures. We study the combination of the functional lifting technique with two different relaxations of the histogram prior and derive a jointly convex variational approach. Mathematical equivalence of both relaxations is established and cases where optimality holds are discussed. Additionally, we present an efficient algorithmic scheme for the numerical treatment of the presented model. Experiments using the basic total-variation based denoising approach as a case study demonstrate our novel regularization approach.}, 
location = {}, 
keywords = {ams subject classifications; 1; introduction; convex optimization; 10; 1137; 120897535; 65d18; 65k10; 68t45; 68u10; 90c06; 90c25; a broad range of; doi; low-level image; powerful variational approaches to; variational image processing; wasserstein distance}}


@Article{Xin2019,
author = {Xin, Shumian and Nousias, Sotiris and Kutulakos, Kiriakos N and Sankaranarayanan, Aswin C and Narasimhan, Srinivasa G and Gkioulekas, Ioannis}, 
title = {A theory of Fermat paths for non-line-of-sight shape reconstruction}, 
journal = {}, 
volume = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 
number = {}, 
pages = {6800–6809}, 
year = {2019}, 
abstract = {}, 
location = {}, 
keywords = {}}


@inproceedings{Xu2017,
author = {Xu, Dan and Ricci, Elisa and Ouyang, Wanli and Wang, Xiaogang and Sebe, Nicu}, 
editor = {}, 
title = {Multi-scale continuous crfs as sequential deep networks for monocular depth estimation}, 
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 
publisher = {}, 
address = {}, 
pages = {5354-5362}, 
year = {2017}, 
}

@inproceedings{Xu2018,
author = {Xu, Dan and Wang, Wei and Tang, Hao and Liu, Hong and Sebe, Nicu and Ricci, Elisa}, 
editor = {}, 
title = {Structured attention guided convolutional neural fields for monocular depth estimation}, 
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 
publisher = {}, 
address = {}, 
pages = {3917-3925}, 
year = {2018}, 
abstract = {Recent works have shown the benefit of integrating Conditional Random Fields (CRFs) models into deep architectures for improving pixel-level prediction tasks. Following this line of research, in this paper we introduce a novel approach for monocular depth estimation …}, 
keywords = {}}

@inproceedings{Zoran2015,
author = {Zoran, Daniel and Isola, Phillip and Krishnan, Dilip and Freeman, William T.}, 
editor = {}, 
title = {Learning Ordinal Relationships for Mid-Level Vision}, 
booktitle = {IEEE International Conference on Computer Vision (ICCV)}, 
year = {2015}, 
abstract = {}, 
keywords = {}}

@Article{Zhang2017,
author = {Zhang, Richard and Zhu, Jun-Yan and Isola, Phillip and Geng, Xinyang and Lin, Angela S and Yu, Tianhe and Efros, Alexei A}, 
title = {Real-Time User-Guided Image Colorization with Learned Deep Priors}, 
journal = {ACM Transactions on Graphics (TOG)}, 
volume = {9}, 
number = {4}, 
pages = {}, 
year = {2017}, 
abstract = {}, 
location = {}, 
keywords = {}}


@Article{Zhang2018,
author = {Zhang, C and Lindner, S and Antolovic, IM and Wolf, M and Charbon, E}, 
title = {A CMOS SPAD Imager with Collision Detection and 128 Dynamically Reallocating TDCs for Single-Photon Counting and 3D Time-of-Flight Imaging.}, 
journal = {Sensors (Basel)}, 
volume = {18}, 
number = {11}, 
pages = {}, 
year = {2018}, 
abstract = {Per-pixel time-to-digital converter (TDC) architectures have been exploited by single-photon avalanche diode (SPAD) sensors to achieve high photon throughput, but at the expense of fill factor, pixel pitch and readout efficiency. In contrast, TDC sharing architecture usually features high fill factor at small pixel pitch and energy efficient event-driven readout. While the photon throughput is not necessarily lower than that of per-pixel TDC architectures, since the throughput is not only decided by the TDC number but also the readout bandwidth. In this paper, a SPAD sensor with 32 × 32 pixels fabricated with a 180 nm CMOS image sensor technology is presented, where dynamically reallocating TDCs were implemented to achieve the same photon throughput as that of per-pixel TDCs. Each 4 TDCs are shared by 32 pixels via a collision detection bus, which enables a fill factor of 28% with a pixel pitch of 28.5 μm. The TDCs were characterized, obtaining the peak-to-peak differential and integral non-linearity of -0.07/+0.08 LSB and -0.38/+0.75 LSB, respectively. The sensor was demonstrated in a scanning light-detection-and-ranging (LiDAR) system equipped with an ultra-low power laser, achieving depth imaging up to 10 m at 6 frames/s with a resolution of 64 × 64 with 50 lux background light.}, 
location = {Quantum and Computer Engineering, Delft University of Technology, Mekelweg 4, 2628CD Delft, The Netherlands. c.zhang-10@tudelft.nl. Biomedical Optics Research Laboratory, University of Zurich, Rämistrasse 71, 8006 Zürich, Switzerland. scott.lindner@epfl.ch. Advanced Quantum Architecture Laboratory, École Polytechnique Fédérale de Lausanne (EPFL), Route Cantonale, 1015 Lausanne, Switzerland. scott.lindner@epfl.ch. Advanced Quantum Architecture Laboratory, École Polytechnique Fédérale de Lausanne (EPFL), Route Cantonale, 1015 Lausanne, Switzerland. michel.antolovic@epfl.ch. Biomedical Optics Research Laboratory, University of Zurich, Rämistrasse 71, 8006 Zürich, Switzerland. martin.wolf@usz.ch. Advanced Quantum Architecture Laboratory, École Polytechnique Fédérale de Lausanne (EPFL), Route Cantonale, 1015 Lausanne, Switzerland. edoardo.charbon@epfl.ch. Kavli Institute of Nanoscience, 2628CJ Delft, The Netherlands. edoardo.charbon@epfl.ch.}, 
keywords = {}}


@Article{Veerappan2011,
author = {Veerappan, Chockalingam and Richardson, Justin and Walker, Richard and Li, Day-Uey and Fishburn, Matthew W and Maruyama, Yuki and Stoppa, David and Borghetti, Fausto and Gersbach, Marek and Henderson, Robert K}, 
title = {A $160 \times 128$ single-photon image sensor with on-pixel 55ps 10b time-to-digital converter}, 
journal = {}, 
volume = {2011 IEEE International Solid-State Circuits Conference}, 
number = {}, 
pages = {312–314}, 
year = {2011}, 
abstract = {}, 
location = {}, 
keywords = {}}


@Article{Hoiem2005,
author = {Hoiem, Derek and Efros, Alexei A and Hebert, Martial}, 
title = {Automatic photo pop-up}, 
journal = {}, 
volume = {ACM transactions on graphics}, 
number = {TOG}, 
pages = {577–584}, 
year = {2005}, 
abstract = {This paper presents a fully automatic method for creating a 3D model from a single photograph. The model is made up of several texture-mapped planar billboards and has the complexity of a typical children’s pop-up book illustration. Our main insight is that instead of …}, 
location = {}, 
keywords = {}}

@article{geiger2013vision,
  title={Vision meets robotics: The KITTI dataset},
  author={Geiger, Andreas and Lenz, Philip and Stiller, Christoph and Urtasun, Raquel},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  pages={1231--1237},
  year={2013},
  publisher={Sage Publications Sage UK: London, England}
}


@inproceedings{ren2012rgb,
  title={Rgb-(d) scene labeling: Features and algorithms},
  author={Ren, Xiaofeng and Bo, Liefeng and Fox, Dieter},
  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2759--2766},
  year={2012},
  organization={IEEE}
}

@inproceedings{silberman2012indoor,
  title={Indoor segmentation and support inference from rgbd images},
  author={Silberman, Nathan and Hoiem, Derek and Kohli, Pushmeet and Fergus, Rob},
  booktitle={European Conference on Computer Vision},
  pages={746--760},
  year={2012},
  organization={Springer}
}

@inproceedings{gupta2013perceptual,
  title={Perceptual organization and recognition of indoor scenes from RGB-D images},
  author={Gupta, Saurabh and Arbelaez, Pablo and Malik, Jitendra},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={564--571},
  year={2013}
}

@inproceedings{shrivastava2013building,
  title={Building part-based object detectors via 3d geometry},
  author={Shrivastava, Abhinav and Gupta, Abhinav},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1745--1752},
  year={2013}
}

@inproceedings{lin2013holistic,
  title={Holistic scene understanding for 3d object detection with rgbd cameras},
  author={Lin, Dahua and Fidler, Sanja and Urtasun, Raquel},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1417--1424},
  year={2013}
}

@inproceedings{gupta2014learning,
  title={Learning rich features from RGB-D images for object detection and segmentation},
  author={Gupta, Saurabh and Girshick, Ross and Arbel{\'a}ez, Pablo and Malik, Jitendra},
  booktitle={European Conference on Computer Vision},
  pages={345--360},
  year={2014},
  organization={Springer}
}

@inproceedings{song2014sliding,
  title={Sliding shapes for 3d object detection in depth images},
  author={Song, Shuran and Xiao, Jianxiong},
  booktitle={European conference on computer vision},
  pages={634--651},
  year={2014},
  organization={Springer}
}


@inproceedings{song2016deep,
  title={Deep sliding shapes for amodal 3d object detection in rgb-d images},
  author={Song, Shuran and Xiao, Jianxiong},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={808--816},
  year={2016}
}

@inproceedings{wu20153d,
  title={3d shapenets: A deep representation for volumetric shapes},
  author={Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1912--1920},
  year={2015}
}

@inproceedings{maturana2015voxnet,
  title={Voxnet: A 3d convolutional neural network for real-time object recognition},
  author={Maturana, Daniel and Scherer, Sebastian},
  booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={922--928},
  year={2015},
  organization={IEEE}
}

@inproceedings{qi2016volumetric,
  title={Volumetric and multi-view cnns for object classification on 3d data},
  author={Qi, Charles R and Su, Hao and Nie{\ss}ner, Matthias and Dai, Angela and Yan, Mengyuan and Guibas, Leonidas J},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5648--5656},
  year={2016}
}

@inproceedings{zhang2013estimating,
  title={Estimating the 3d layout of indoor scenes and its clutter from depth sensors},
  author={Zhang, Jian and Kan, Chen and Schwing, Alexander G and Urtasun, Raquel},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1273--1280},
  year={2013}
}

@article{pawlikowska2017single,
  title={Single-photon three-dimensional imaging at up to 10 kilometers range},
  author={Pawlikowska, Agata M and Halimi, Abderrahim and Lamb, Robert A and Buller, Gerald S},
  journal={Optics express},
  volume={25},
  number={10},
  pages={11919--11931},
  year={2017},
}

@article{Li:2019,
  title={Single-photon computational 3D imaging at 45 km},
  author={Li, Zheng-Ping and Huang, Xin and Cao, Yuan and Wang, Bin and Li, Yu-Huai and Jin, Weijie and Yu, Chao and Zhang, Jun and Zhang, Qiang and Peng, Cheng-Zhi and others},
  journal={arXiv preprint arXiv:1904.10341},
  year={2019}
}

@article{Kirmani:2014,
  title={First-photon imaging},
  author={Kirmani, Ahmed and Venkatraman, Dheera and Shin, Dongeek and Cola{\c{c}}o, Andrea and Wong, Franco NC and Shapiro, Jeffrey H and Goyal, Vivek K},
  journal={Science},
  volume={343},
  number={6166},
  pages={58--61},
  year={2014},
}

@article{Lasinger:2019,
  title={Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer},
  author={Katrin Lasinger and René Ranftl and Konrad Schindler and Vladlen Koltun},
  journal={arXiv:1907.01341},
  year={2019}
}

@inproceedings{burri2016linospad,
  title={LinoSPAD: a time-resolved $256 \times 1$ CMOS SPAD line sensor system featuring 64 FPGA-based TDC channels running at up to 8.5 giga-events per second},
  author={Burri, Samuel and Homulle, Harald and Bruschini, Claudio and Charbon, Edoardo},
  booktitle={Optical Sensing and Detection IV},
  volume={9899},
  pages={98990D},
  year={2016},
  organization={International Society for Optics and Photonics}
}

@article{burri2017linospad,
  title={LinoSPAD: A compact linear SPAD camera system with 64 FPGA-based TDC modules for versatile 50 ps resolution time-resolved imaging},
  author={Burri, Samuel and Bruschini, Claudio and Charbon, Edoardo},
  journal={Instruments},
  volume={1},
  number={1},
  pages={6},
  year={2017},
  publisher={Multidisciplinary Digital Publishing Institute}
}