\begin{figure}[t!]
	\centering
  \includegraphics[width=0.8\columnwidth]{prototype_single_col.pdf}
  \caption{Prototype scanning setup. The pulsed light from the laser travels
    through a beam splitter before being guided by the galvo to the scene.
    Returning light is measured by the single-pixel SPAD. The Kinect v2 RGB 
    camera is used to capture the image used to generate the monocular depth estimate
    (the depth camera is not used).}
  \label{fig:prototype}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Prototype RGB-SPAD Camera Hardware}

% The laser operates at 450~nm with a pulse repetition rate of 25~MHz with a peak power of 450~mW and average power of 0.5~mW.

As shown in Figure~\ref{fig:prototype}, our prototype comprises a color camera (Microsoft Kinect v2), a single-pixel SPAD (Micro Photon Devices 100~$\mu m$ PDM series, free-running), a laser (ALPHALAS PICOPOWER-LD-450-50), and a two-axis galvanometer mirror system (Thorlabs GVS012). The laser operates at 670~nm with a pulse repetition rate of 10~MHz with a peak power of 450~mW and average power of 0.5~mW. 

The SPAD records temporal histograms with 4096 bins, each corresponding to a time window of 16~ps. The SPAD and laser are co-axially aligned using a beam splitter (Thorlabs PBS251). The full width at half maximum (FWHM) of the combined laser pulse width and SPAD jitter is about 70~ps, allowing the system to record depth map with an accuracy of about 1~cm. A National Instruments data acquisition device (NI-DAQ USB-6343) provides synchronization signals for the galvos, SPAD, and laser. The ground truth depth map is raster-scanned at a resolution of $512 \times 512$ pixels, and the single-pixel, diffused SPAD measurement is generated by summing all of these measurements for a specific scene. This allows us to validate the accuracy of the proposed histogram matching algorithm, which only uses the integrated single histogram, by comparing it with the captured depth. The monocular depth estimate is calculated using the RGB image captured by the Kinect v2.

We determined camera intrinsics and extrinsics for the Kinect's RGB camera and
the scanning system using MATLAB's camera calibration toolbox. 
The SPAD histogram and RGB image were captured from slightly different viewpoints; we account for this in the SPAD histogram by shifting the 1D transient according to the SPAD's offset from the RGB camera. 
%We use the camera extrinsics to Once the SPAD volume has been summed over the spatial dimensions, it is unclear
%how to warp the resulting 1D transient to the RGB camera's perspective (in a
%single-pixel flash LiDAR setup, the $512 \times 512$ ground truth data is not
%available). However, we do shift the 1D transient by the $z$ displacement
%between the SPAD and the RGB camera to partially account for this.

We re-bin the captured 1D transient for the indoor captured results 
using Equation~\ref{eq:sid_bin_edges} with $K = 600$ bins, and $(\ell, u)=(0.4, 9.)$.
For the outdoor captured result, we use $K=600$ and $(\ell, u)=(0.4, 11)$. 
%% \textcolor{red}{Mark, please write a short paragraph on calibration details, including any warping of the SPAD histograms you did to compensate for the offset in camera and SPAD position.}
%
%Missing:
%%
%\begin{itemize}
%\item RGB resolution used for MDE
%\item do we also have Kinect depth maps for comparison? (yes) kinect resolution: RGB is 1920x1080 and depth camera is 512x424
%\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experimental Results}

Using the hardware prototype, we captured a number of scenes as shown in
Figure~\ref{fig:results_captured} and in the supplement. We crop the RGB image
to have dimensions that are multiples of 32. For DORN only, we further
downsample the image to a resolution of $353 \times 257$. We then feed this RGB
image into the monocular depth estimation algorithm. In Figure~\ref{fig:results_captured} we show a subset of the scenes we captured and processed with MiDAS~\cite{Lasinger:2019}, which achieved the best results among the depth estimators we tested. Additional scenes, also processed with other MDE approaches, including DenseDepth~\cite{Alhashim2018} and DORN~\cite{Fu2018}, are included in the supplement. The ground truth depth is captured with the scanned SPAD, as described above, and regions with low signal-to-noise ratio are masked out (shown in black).

In the first two examples, the ``Hallway'' and ``Conference Room'' scenes, we see that the monocular depth CNN estimates the ordinal depth of the scene reasonably well. However, the root mean squared error (RMSE) for these two scenes is relatively high ranging from 2.6--3.2~m (see red/white error maps in Fig.~\ref{fig:results_captured}). The proposed method using a single diffused SPAD measurement corrects this systematic depth estimation error and brings the RMSE down to 0.6--0.9~m. The ``Poster'' scene is meant to confuse the CNN---it shows a flat poster with a printed scene. As expected, the CNN predicts that the statue is closer than the arches in the background, which is incorrect in this case. The proposed method uses the SPAD histogram to correctly flatten the estimated depth map. 

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{captured.pdf}
	\caption{Experimental results. For each scene, we record a ground truth depth map that is raster-scanned with the SPAD (upper left subimages), and an RGB image (lower left subimages). A monocular depth CNN predicts an initial depth map (top middle subimages), which is corrected with the diffused SPAD histogram using the proposed method (top right subimages). As shown by the error maps and root mean squared error (RMSE) for each example (lower center, right subimages). The CNN is confused when we show it a photograph of a poster (bottom scene); it incorrectly predicts the depth of the scene depicted on the flat print. Our method is able to correct this error. }
	\label{fig:results_captured}
\end{figure}
