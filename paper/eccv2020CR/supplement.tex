% updated April 2002 by Antje Endemann
% Based on CVPR 07 and LNCS, with modifications by DAF, AZ and elle, 2008 and AA, 2010, and CC, 2011; TT, 2014; AAS, 2016; AAS, 2020

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{color}

% INITIAL SUBMISSION - The following two lines are NOT commented
% CAMERA READY - Comment OUT the following two lines
% \usepackage{ruler}
% \usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}


\usepackage{booktabs}
\usepackage{caption}
\usepackage{array}
\usepackage{tabularx}
\usepackage{bm}
\usepackage{multirow}
\usepackage{float}
\usepackage[utf8x]{inputenc}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algorithmicx}
\usepackage{dblfloatfix}

\input{mark_defs.tex}
% \DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
% \def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g.}} \def\Eg{\emph{E.g.}}
\def\ie{\emph{i.e.}} \def\Ie{\emph{I.e.}}
\def\cf{\emph{c.f.}} \def\Cf{\emph{C.f.}}
\def\etc{\emph{etc}} \def\vs{\emph{vs.}}
\def\wrt{w.r.t.} \def\dof{d.o.f.}
\def\etal{\emph{et al.}}

\newcommand{\edit}[1]{\textcolor{black}{#1}}

\graphicspath{{figures/}{"G:/Shared drives/Stanford Computational
    Imaging/Projects/single_spad_depth/figures/"}{"H:/Shared drives/Stanford Computational
    Imaging/Projects/single_spad_depth/figures/"}{"/Users/gordon/mount/teamdrive/Shared
    drives/Stanford Computational Imaging/Projects/single_spad_depth/figures/"}
  {"/Volumes/GoogleDrive/Shared drives/Stanford Computational Imaging/Projects/single_spad_depth/figures/"}}
% Pages are numbered in submission mode, and unnumbered in camera-ready

\begin{document}
% \renewcommand\thelinenumber{\color[rgb]{0.2,0.5,0.8}\normalfont\sffamily\scriptsize\arabic{linenumber}\color[rgb]{0,0,0}}
% \renewcommand\makeLineNumber {\hss\thelinenumber\ \hspace{6mm} \rlap{\hskip\textwidth\ \hspace{6.5mm}\thelinenumber}}
% \linenumbers
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{3668}  % Insert your submission number here

%\title{Improving Monocular Depth Estimation with Global Depth Histogram Matching using a Single SPAD Transient} % Replace with your title
\title{\textcolor{black}{Disambiguating Monocular Depth Estimation with a Single
    Transient: Supplemental Information}} % Replace with your title

% INITIAL SUBMISSION 
%\begin{comment}
% \titlerunning{ECCV-20 submission ID \ECCVSubNumber}
% \authorrunning{ECCV-20 submission ID \ECCVSubNumber}
% \author{Anonymous ECCV submission}
% \institute{Paper ID \ECCVSubNumber}
%\end{comment}
%******************

% CAMERA READY SUBMISSION
% \begin{comment}
\titlerunning{Disambiguating Monocular Depth Estimation with a Single Transient}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Mark Nishimura  \and
David B. Lindell \and
Christopher A. Metzler \and \\
Gordon Wetzstein}

%
\authorrunning{M. Nishimura et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Stanford University, Stanford, CA \\
\email{\{markn1, lindell, cmetzler, gordon.wetzstein\}@stanford.edu}}
% \end{comment}
%******************
\maketitle
% \begin{abstract}
%   TODO: Abstract
% \end{abstract}


\section{Hardware prototype details}
% show this in supplement
The monocular depth estimate is calculated using the RGB image captured by the
Kinect v2. The SPAD records temporal histograms with 4096 bins, each
corresponding to a time window of 16~ps. The SPAD and laser are co-axially
aligned using a beam splitter (Thorlabs PBS251). The full width at half maximum
(FWHM) of the combined laser pulse width and SPAD jitter is about 70~ps,
allowing the system to record depth maps with an accuracy of about 1~cm. A
National Instruments data acquisition device (NI-DAQ USB-6343) provides
synchronization signals for the galvos, SPAD, and laser. The ground truth depth
map is raster-scanned at a resolution of $512 \times 512$ pixels, and the
single-pixel, diffused SPAD measurement is generated by summing all of these
measurements for a specific scene. This allows us to validate the accuracy of
the proposed histogram matching algorithm, which only uses the integrated single
histogram, by comparing it with the captured depth---such validation would not
be possible if we were to capture measurements with an optically diffused SPAD.

\section{Comparison of diffused vs. scanned imaging}
In our experiments, we capture measurements by scanning the scene with a
single-pixel SPAD detector whose optical path is aligned with a laser. This
arrangement allows us to capture a reference ``ground truth'' depth map for
quantitative validation of our method. To emulate measurements captured using a
system where the laser and detector are diffused over the scene, we digitally
sum the measurements to obtain a single transient. 

%To verify that our digitally aggregated scanned SPAD measurements match measurements produced by an optically diffused SPAD, we set up a slightly modified version of our prototype consisting of both scanned and diffused SPADs side-by-side. Details about this system can be found in the supplement. We then captured measurements of the simple scene shown in Figure~\ref{fig:hardware}(b) with both SPADs. The aggregated measurements from the scanned SPAD are shown alongside the optically diffused SPAD's measurements in Figure ~\ref{fig:hardware}(c). These results demonstrate that the two systems produce near equivalent measurements. Slight differences in their two histograms can be attributed to a baseline difference of about 10cm between the SPAD positions. That is, they observe scene from slightly different perspectives.
	%NOTE:JUSTIFY USE OF DIFFERENT PROTOTYPE FOR RESULTS IN THE WILD WITH EYE SAFETY?} \edit{We show proof-of-concept results captured by this system in   Figure~\ref{fig:captured_diffuse_small}. For}

In order to verify that digital summation of scanned measurements yields results
that are similar to those captured by a diffused laser and detector, we capture
an example scene using a modified hardware prototype in both scanned and
diffused modes. This hardware prototype (shown in Fig.~\ref{fig:hardware}) is less mobile than 
our unmodified prototype (which was able capture a wider variety of scenes,
including outdoors, along with their ground truth
depths), but allows us to use a more powerful laser (Katana 05HP, 532 nm)
operated at approximately 25 mW average power. We also use two single-pixel SPAD
detectors, where one SPAD is aligned with the optical path of the laser, and the
other SPAD is operated without a lens to integrate light from the entire scene.
Both SPADs are fitted with a 10 nm bandpass filter centered at 532 nm, which
reduces the amount of integrated ambient light. We attach a holographic diffuser
(Thorlabs ED1-S50) to the laser output in order to diffuse light onto the scene.
Alternatively, we remove the diffuser and use a pair of scanning mirrors to scan
the scene.

The modified hardware setup is used to capture an example scene in both scanned and
diffused modes, and the resulting transients are used to refine an initial depth
estimate from the Kinect RGB image. We illustrate the results of this procedure
in Fig.~\ref{fig:comparison}. The reconstructions from the scanned and diffused
measurements are similar in reconstruction quality and also show similar
quantitative improvement in terms of error over the initial depth estimate. The
unnormalized photon counts are also shown in Fig.~\ref{fig:comparison}, and we
note that the counts show similar trends. The number of recorded photons in
these experiments is shown in Table~\ref{tab:photon_counts}. In both cases, the
rate of detected photons is far less ($<$5\%) than the number of emitted laser
pulses, and so we conclude that the measurements are captured in the low-flux
regime where pileup effects are negligible. We attribute most of the differences
between the scanned and diffused transients to the approximately 16 cm vertical
baseline between the positions of the diffused and scanned SPADs (see
Fig.~\ref{fig:hardware}).
\begin{figure}[ht!]
  \centering \includegraphics[width=0.8\textwidth]{diffused_hardware/setup.pdf}
  \caption{Modified hardware setup. The setup is used to compare scanned and
  diffused measurements and employs two SPAD detectors and two laser
  configurations. In the first configuration, the scene is illuminated by
  sending the laser light through a holographic diffuser and a lensless SPAD
  integrates light from the entire scene. In the second, the SPAD is aligned
  with the optical path of the laser and the scene is scanned using a pair of
  scanning mirrors. The baseline between the two SPADs (right) results in some
  observed differences in the recorded transients.}
  \label{fig:hardware}
\end{figure}

\begin{figure}[ht!]
  \centering \includegraphics[width=\textwidth]{captured_diffuse.pdf}
  \caption{Comparison of scan + sum and diffused SPAD. The transients are captured
      with the same total exposure time and are qualitatively similar without 
      noticeable pileup effects. We use $K = 300$ bins for the reconstruction
      and a depth range of $[0.8, 6]$ meters. MiDaS \cite{Lasinger:2019} does
      not produce globally-scaled depth, so we scale it to fit this depth range.
      We inpaint the depth map from the Kinect's depth camera to acquire ground
      truth depth.}
  \label{fig:comparison}
\end{figure}

\begin{table}[ht!]
    \renewcommand{\arraystretch}{1.3}
    \setlength{\tabcolsep}{10pt}
     \centering
     \begin{tabular}{lccc} \toprule
         \textbf{Experiment} & \textbf{Detected Photons} & \textbf{Laser Pulses} & \textbf{Detection Rate} \\\hline
         \textbf{Scanned} & $1.4\times10^7$ & $6\times10^8$ & $2.3\%$ \\
         \textbf{Diffused} & $2.4\times10^7$ & $6\times10^8$ & $4.0\%$ \\
        \bottomrule
    \end{tabular}
    \vspace{0.4em}
    \caption{\textbf{Recorded photons for diffused vs. scanned scene.} In each capture mode, scanned or diffused, the number of detected photons does not exceed 5\% of the number of emitted laser pulses, placing the capture within the low-flux regime where pileup effects are negligible.}
    \label{tab:photon_counts}
\end{table}

\section{Radiometric calculation}
Assuming an indoor scene with fluorescent bulbs and an ambient spectral irradiance of $I_A =
1~\text{mW}/\text{m}^2$ (across the 1 nm pass band of a spectral filter matched
to the laser), we find that the laser power required to achieve a minimum SBR of
5 for a diffuse scene at $R = 3$ m and a field of view of
$\theta = 40^\circ$ can be calculated as
%
\begin{equation}
  P_{\text{min}} = I_A \cdot 4 R^2 \tan^2(\theta/2) \cdot SBR_{\text{min}},
\end{equation}
giving $P_{\text{min}}=21$ mW. We note that this is significantly
less than the 60 mW used by the Kinect sensor to diffusely illuminate a scene.

Under these scene parameters, we can compute the total incident
flux on the detector per second (derived in~\cite{mcmanamon2012ladar}) as
\begin{equation}
  P_R = P_T \cdot \rho \cdot \frac{A_{rec}}{\pi R^2} \cdot \eta
\end{equation}
where $P_T$ is the illumination power in the visible region, $\rho$ is its
albedo, $A_{rec}$ is the area of the detector region, $R$ is the distance to the
object, and $\eta$ is the quantum efficiency of the detector.
% For a range of
% values similar to those of our experiments, we find
% that the ratio of photon detections to illumination pulses is roughly 4\%, or
% well within low-flux regime where SPAD pileup is negligible~\cite{oconnortcspc}.

For scene itself, we assume a vertical, planar, perfectly Lambertian surface. The following table gives the values used for this calculation.
\begin{figure}[H]
  \centering
  \begin{tabular}{c|c|c}
    \toprule
    Symbol & Description & Nominal Value \\
    \midrule
    $\rho$  & Albedo of lambertian surface & $0.3$ \\
    $P_T$ & Total irradiance at wavelength (W/$\text{m}^2$) & $0.026$ \\
    $R$ & Distance to surface (m) & $3$ \\
    $A_{rec}$ & Area of detector ($\text{m}^2$) & $1.96 \times 10^{-9}$ \\
    $\eta$ & Quantum efficiency of detector & $0.3$ \\
    $P_R$ & Received power at detector (W) & $1.62 \times 10^{-13}$ \\
    \bottomrule
  \end{tabular}
  \caption{Table of nominal values for radiometric calculation.}
  \label{fig:sbr_calculation}
\end{figure}
Once $P_R$ is determined, we compute the number of photons using the laser
wavelength $\lambda = 532$ nm as
\begin{equation}
  N = \frac{P_R\lambda}{hc}
\end{equation}
where $h \approx 6.626 \times 10^{-34}$ $\text{J} \cdot \text{s}$ is Planck's constant and $c \approx 3
\times 10^8$ $\text{m}/\text{s}$ is the speed of light. Using the fact that our laser runs at 10
MHz, we get the number of photons per pulse as $0.043$ or $4.3\%$, which puts us in the
low-flux regime (where photons per pulse is $< 5\%$)~\cite{oconnortcspc}.

\section{Ablation study on number of SID bins}
% \begin{itemize}
%   \item Simulation: 70, 140, 210, 280 bins
%   \item Captured: 70, 140, 210, 280 bins
% \end{itemize}
% Compare RMSE and runtime.
We conducted an ablation study on the effect of the number of SID bins
\cite{Fu2018} on both runtime and RMSE. We performed this analysis using SPAD
data with a signal-to-background (SBR) of 100, simulated on the test set of NYU
Depth v2. We used DenseDepth~\cite{Alhashim2018} for our MDE CNN.
Only the histogram matching portion was timed, not the CNN nor the
denoising pipeline.

\begin{figure}[H]
  \centering
  \begin{tabular}{c|cc}
    \toprule
    \# of sid bins & RMSE & Approx. Time/image (sec) \\
    \midrule
    70  & $0.351$ & $0.24$ \\
    140 & $0.346$ & $0.63$ \\
    210 & $0.345$ & $1.12$ \\
    280 & $0.345$ & $1.84$ \\
    \bottomrule
  \end{tabular}
  \caption{Effect of number of SID bins on RMSE and runtime. The marginal
    improvement in RMSE is offset by the increase in runtime as the number of
    bins grows.}
  \label{fig:sid_ablation}
\end{figure}


\section{Ablation study on effect of reflectance estimation}
We conducted an ablation study on whether the use of a reflectance
estimate has an impact on the runtime and quality of the solution.
We performed this analysis using SPAD
data with a signal-to-background (SBR) of 100, simulated on the test set of NYU
Depth v2 and using DenseDepth~\cite{Alhashim2018} for our MDE CNN.
Only the histogram matching portion was timed, not the CNN nor the denoising
pipeline. Using the intensity to produce the initial weighted histogram
$h_{\text{source}} $ provides noticable improvements
in RMSE, but intensity may safely be ignored during the pixel movement step,
resulting in noticeable speed improvements.

% \begin{itemize}
%   \item Intensity used to initialize histogram, not used to move pixels
%   \item Intensity used to initialize histogram and move pixels
%   \item Intensity not used to initialize histogram, used to move pixels
%   \item Intensity not used to initialize histogram, not used to move pixels
% \end{itemize}
% Compare RMSE and runtime.
\begin{figure}[H]
  \centering
  \begin{tabular}{c|c|c|c}
    \toprule
    Intensity-weighted & Intensity-aware & Avg. & Time \\ histogram & pixel movement & RMSE & per image (sec) \\
    \midrule
    \multirow{2}{*}{Yes} & Yes & $0.346$ & $4.6$ \\
                         & No & $0.346$ & $0.6$ \\
    \midrule
    \multirow{2}{*}{No} & Yes & $0.444$ & $4.7$ \\
                        & No & $0.444$ & $0.6$ \\
    \bottomrule
  \end{tabular}
  \caption{Effect of reflectance modeling on RMSE and runtime. When the SPAD
    is simulated with the reflectance info but no reflectance estimate is used
    to generate a weighted histogram from the CNN depth map, the results are
    significantly worse. Furthermore, once the pixel movement matrix has been
    computed, the pixel movement procedure need not take
    into account the weights of the pixels being moved, since doing so provides no
    improvement and can take appreciably longer than a vectorized implementation
    that does not take pixel weights into account.}
\end{figure}

\section{Pseudocode, pixel shifting, and dither artifacts}
We give pseudocode for our algorithm here. In the first part of our algorithm we
compute the pixel shifting matrix mapping the histogram $h_s$ (computed from the
initial depth map and reflectance estimate)to $h_t$ (computed from the captured
transient).
{ \footnotesize
\begin{algorithm}
 \caption{Find Pixel Movement} 
 \label{alg:ehm}
 \begin{algorithmic}
	\footnotesize
  \Procedure{FindPixelMovement}{$h_s$ of length $M$, $h_t$ of length $N$} 
    \State Initialize $T$ as an $M \times N$ array of zeros.
    \For{$m$ in $1,\ldots,M$}
      \For{$n$ in $1,\ldots,N$}
        \State $p_s \gets \sum_{i=1}^{n-1} T[m, i]$
        \State $p_t \gets \sum_{i=1}^{m-1} T[i, n]$
        \State $T[m, n] \gets \min(h_s[m] - p_s, h_t[n] - p_t)$
      \EndFor
    \EndFor
    \State \textbf{return} $T$
  \EndProcedure
 \end{algorithmic}
\end{algorithm}
}

Given this pixel movement matrix $T$, we apply the appropriate movements to the
initial depth map $I$. The pixels of the image $I$ take depth bin values in $\set{0,\ldots,K-1}$. 
{ \footnotesize
  \begin{algorithm}[H]
    \caption{Move Pixels}
    \label{alg:move_pixels}
    \begin{algorithmic}
      \footnotesize
      \Procedure{MovePixels}{input image $I$ size $M \times N$,
        pixel movement matrix $T$ of size $K \times K$}
      \For{$k$ in $0,\ldots, K-1$}
        \State $p[k,:] \gets T[k,:]/\sum_{i=1}^K T[k,i]$
      \EndFor
      \For{$m$ in $1,\ldots,M$}
        \For{$n$ in $1,\ldots,N$}
          \State Sample $k'$ according to $p[I[m,n],:]$.
          \State $I[m,n] \gets k'$.
        \EndFor
      \EndFor
      \State \textbf{return} $I$
      \EndProcedure
    \end{algorithmic}
  \end{algorithm}
} Because the pixel shifting process in Algorithm \ref{alg:move_pixels} contains
a sampling step, it is possible for
\textit{dither artifacts} to appear in the output image $I$, as shown in figure
\ref{fig:dither}. Specifically, when there are multiple possible output depth
bins for a given input depth bin, and a large region of equal depth in the input
image, the randomness in the pixel shifting algorithm will distribute the pixels
of large, equal-depth region in the input across the multiple possible output
depth bins in a random fashion.

\begin{figure}[H]
  \centering \includegraphics[width=0.66\textwidth]{dither.pdf}
  \caption{Example of dither artifacts. Sometimes, when our histogram matching
    is applied to images with large regions of similar depths, dither artifacts
    will occur.}
  \label{fig:dither}
\end{figure}

\clearpage
\section{Additional results on NYU Depth v2}
Figures \ref{fig:densedepth_1}--\ref{fig:midas_3} show additional results
for our method on the NYU Depth v2 dataset when the depth estimate is
initialized with the DenseDepth \cite{Alhashim2018} (Figures
\ref{fig:densedepth_1}--\ref{fig:densedepth_3}), DORN \cite{Fu2018} (Figures
\ref{fig:dorn_1}--\ref{fig:dorn_3}) 
and MiDaS \cite{Lasinger:2019} (Figures \ref{fig:midas_1} -- \ref{fig:midas_3}) monocular depth estimators.

We compare the output of the network $z_0$, the
median-rescaled network output (where the depth map $z_0$ is scaled pixel-wise by a
scalar $\frac{\text{median}(z_{GT})}{\text{median}(z_0)}$, $z_{GT}$ being the
ground truth depth map), the network output matched to the ground truth depth histogram, and the output of
our histogram matching method under a signal-to-background ratio (SBR) of 100.
We use the luminance of the RGB image as our reflectance map
for both SPAD simulation and histogram matching.
We show absolute difference maps and also give
the root-mean-square error (RMSE) for each example.

\newpage
\input{sections/supplement/simulation_comparison.tex}

\clearpage
\section{Additional results for hardware prototype}
Figures \ref{fig:midas_captured_1}--\ref{fig:dorn_captured_3} show all the
captured results when the depth estimate is initialized with the MiDaS
\cite{Lasinger:2019} (Figures \ref{fig:midas_captured_1}--\ref{fig:midas_captured_3}),
DenseDepth (Figures \ref{fig:densedepth_captured_1}--\ref{fig:densedepth_captured_3}),
and DORN (Figures \ref{fig:dorn_captured_1}--\ref{fig:dorn_captured_3}). We compare
the output of the network $z_0$, the mean-rescaled network output where the
depth map $z_0$ has been scaled pixel-wise by the scalar
$\frac{\text{median}(h_{target})}{\text{median}(z_0)}$ ($h_{target}$ is the
processed SPAD transient), and the output of our method. As our laser is red, we
use the R channel of the RGB image as our reflectance map. We show absolute
difference maps and also give the root-mean-square-error (RMSE) for each
example.

Black pixels in the ground truth depth correspond to locations where our scanner
was unable to produce an accurate depth estimate (this can occur for a variety
of reasons including dark albedo and surface specularity). These pixels are masked off and not used in the RMSE
calculation, and appear as an absolute difference of 0 in the difference maps.

\newpage

\input{sections/supplement/hardware_comparison.tex}

\clearpage 

{\small
\bibliographystyle{splncs04}
\bibliography{references}
}
\end{document}
