In summary, we demonstrate a method to greatly improve depth estimates from
monocular depth estimators by correcting the scale ambiguity errors inherent
with such techniques. Our approach produces depth maps with accurate absolute
depth, and helps MDE neural networks generalize across scene
types, including on data captured with our hardware prototype.  Moreover, we
require only minimal additional sensing hardware; we show that a single
measurement histogram from a diffused SPAD sensor contains enough information
about global scene geometry to correct errors in monocular depth estimates.

The performance of our method is highly dependent on the accuracy of the initial
depth map of the MDE algorithm. Our results demonstrate that when the MDE
technique produces a depth map with good ordinal accuracy, where the ordering of
object depths is roughly correct, the depth estimate can be corrected to produce
accurate absolute depth. However, if the ordering of the initial depths is not
correct, these errors may propagate to the final output depth map.

In the optically diffused configuration, the laser
power is spread out over the entire scene. Accordingly, for distant scene points
very little light may return to the SPAD, \edit{ making reconstruction difficult
(an analogous problem occurs with dark objects).} \edit{Thus, our method is
best suited to short- to medium-range scenes.}
\edit{On the other hand, in bright environments, pileup will ultimately limit the range of our
  method. However, this can be mitigated with optical elements to reduce the amount
  of incident light, with pileup correction~\cite{Heide:2018,Rapp:2019}, or
  even by taking two transient measurements, one with and one without laser
  illumination, and using their difference to approximate the background-free
  transient.}
\edit{Finally, under normal indoor conditions, it is theoretically possible to
  achieve an SBR of 5 at a range of 3 meters with a laser of only 21 mW while
  remaining in the low-flux regime. We confirm this empirically with our
  diffused setup, which operates without significant pileup effects while using
  approximately 25 mW of laser power (see
  Figure~\ref{fig:captured_diffuse_small} and the supplement for details).}



  %Note
  % that this calculation involves only on the signal-to-background photon ratio,
  % which is independent of the actual number of counts recorded per exposure. In
  % other words, pileup and laser energy concerns may 
  % be addressed entirely independently. As for acquisition time, at a 10 MHz
  % pulse rate with
  % a 5\% photon detection rate, it takes just two seconds to acquire a million
  % photon events.} 

% Furthermore, our system uses a SPAD and is therefore susceptible to pileup.
% Because the SPAD must be quenched before it can
% detect another photon, earlier photons
% from closer objects may systematically prevent detection of photons from further
% objects, causing strong distortion of the transient. Fortunately, at the expense of
% longer acquisition times, pileup can be mitigated
% to a large extent by operating in the low-flux regime
% where the number of photon events per second is controlled to be at most 5\% the
% number of laser pulses per second~\cite{oconnortcspc}. Finally, even in
% the high-flux regime, it is sometimes possible to correct pileup computationally~\cite{Heide:2018}.}

% While our prototype shows results for scanned/summed measurements, our scanning prototype operates in the low-flux regime,
% avoiding pileup. A hypothetical diffused SPAD prototype could also operate in
% this low-flux regime, which would yield identical measurement models and identical
% results. Figure~\ref{fig:hardware}(b)-(c) empirically shows that collecting and summing scanned
% measurements is nearly identical to capturing diffused measurements. ?!We show
% empirical comparisons between the two modes on a more complex scene in the supplement?!}

% Even if diffused and scanned/summed measurements are equivalent
%   computationally, they are not equivalent energetically.  This number is dependent on scene albedo, but all time-of-flight imaging
% systems share this dependency.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Future Work}
\edit{Future work could implement our algorithm or similar sensor fusion
  algorithms on smaller platforms such as existing cell phones with single-pixel SPAD
proximity sensors and RGB cameras. Necessary
  adjustments, such as pairing near-infrared (NIR) SPADs with NIR sensors,
  could be made. The small baseline of such sensors
  would also mitigate the effects of shading and complex BRDFs on the
  reflectance
  estimation step. More sophisticated intrinsic imaging techniques could also be
  employed.}

% \edit{DELETE Learning-based methods to combine the MDE estimates and
% histogram. One might even consider sensing regimes where the number of
% returning signal photons is low, such as when the time-resolved detector and
% camera operate at high framerates. While most MDE techniques are tailored to
% clean RGB images, the transient could be used to help MDE techniques generalize
% to noisy scenes under low-light conditions.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Conclusions}
Since their introduction, monocular depth estimation algorithms have improved
tremendously.  However, recent advances, which have generally relied on new
network architectures or revised training procedures, have produced only modest
performance improvements. In this work we dramatically improve the performance
of several monocular depth estimation algorithms by fusing their estimates with
transient measurements. Such histograms are easy to capture using time-resolved
single-photon detectors and are poised to become an important component of
future low-cost imaging systems.

%Moving forward, sensor fusion is poised to become an even more important component of a variety of vision systems.
%Computational photography approaches may provide the measurements needed to achieve much greater performance improvements, especially as the sensors in devices we use every day become more diverse, complex, and increasingly rely on techniques based on sensor fusion and sophisticated computational algorithms.
 
%\begin{itemize}
%	\item new depth estimation CNNs achieve marginal gains, let's think about computational photography approaches that augment the measurements to achieve better performance
%	\item ...
%\end{itemize}
